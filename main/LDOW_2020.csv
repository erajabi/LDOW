Timestamp,Workshop,Paper title,Link to article,Author-1,Author-1_affiliation,Author-1_country,Author-2,Author-2_affiliation,Author-2_country,Author-3,Author-3_affiliation,Author-3_country,Author-4,Author-4_affiliation,Author-4_country,Author-5,Author-5_affiliation,Author-5_country,Author-6,Author-6_affiliation,Author-6_country,Author-7,Author-7_affiliation,Author-7_country,Paper abstract,Paper Keyword,Paper Categories
12/26/2020 20:05:23,2008,Weaving SIOC into the Web of Linked Data,http://events.linkeddata.org/ldow2008/papers/01-bojars-passant-weaving-sioc.pdf,Uldis Bojars,Digital Enterprise Research Institute,Ireland,Alexandre Passant,Electricité de France, France,Richard Cyganiak,Digital Enterprise Research Institute , Ireland,John Breslin,Digital Enterprise Research Institute , Ireland,,,,,,,,,,"Social media sites can act as a rich source of large amounts
of data by letting anyone easily create content on the Web.
The SIOC ontology and tools developed for it allow us to express this information as interlinked RDF data. This paper
describes approaches for making this Social Web information
an integral part of the Web of Linked Data. ",,
12/26/2020 20:14:32,2008,"SEMANTIC MARC, MARC 21 AND THE SEMANTIC WEB ",http://events.linkeddata.org/ldow2008/papers/02-styles-ayers-semantic-marc.pdf,Rob Styles,Knights Court, United Kingdom,Danny Ayers,Knights Court, United Kingdom,Nadeem Shabir,Knights Court, United Kingdom,,,,,,,,,,,,,"The MARC standard for exchanging bibliographic data has been
in use for several decades and is used by major libraries
worldwide. This paper discusses the possibilities of representing
the most prevalent form of MARC, MARC21, as RDF for the
Semantic Web, and aims to understand the tradeoffs, if any,
resulting from transforming the data. Critically our approach goes
beyond a simple transliteration of the MARC21 record syntax to
develop rich semantic descriptions of the varied things which may
be described using bibliographic records. We present an
algorithmic approach for consistently generating URIs from
textual data, discuss the algorithmic matching of author names
and suggest how RDF generated from MARC records may be
linked to other data sources on the Web.","MARC, MARC21, RDF, Semantic Web, Data Conversion, Inferred Semantics. ",
12/26/2020 20:17:15,2008,The OAI2LOD Server: Exposing OAI-PMH Metadata as Linked Data,http://events.linkeddata.org/ldow2008/papers/03-haslhofer-schandl-oai2lod-server.pdf,Bernhard Haslh,University of Vienna, Austria,Bernhard Schandl,University of Vienna, Austria,,,,,,,,,,,,,,,,"Many institutions grant access to their metadata repositories
via the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). However, this protocol has two significant drawbacks: it does not make its resources accessible via
dereferencable URIs, and it provides only restricted means
of selective access to metadata. The OAI2LOD Server handles these shortcomings by republishing metadata originating from an OAI-PMH endpoint according to the principles
of Linked Data. As the ongoing OAI-ORE specification process shows, these principles are gaining growing importance
also in the digital libraries domain.",,
12/26/2020 20:20:14,2008,A Case Study on Linked Data Generation and Consumption,http://events.linkeddata.org/ldow2008/papers/04-li-zhao-linked-data-generation.pdf,Jianqiang LI,NEC Laboratories, China,Yu ZHAO,NEC Laboratories, China,,,,,,,,,,,,,,,,"The availability of large amounts of interlinked semantic data is a
fundamental prerequisite of the Semantic Web. At present, almost
all the usable ontological data is built manually or by directly
transforming certain (semi-)structured data sources into certain
formats of semantic data. To solve the ‚Äúisolated data island‚Äù
problem of the Semantic Web caused by this situation, the
Linking Open Data initiative was launched to unite these machine
readable datasets as a Web of Data. In this paper, an alternative
method to make a contribution to this trend is proposed. By
extracting the linked data from the existing Web of Documents in
an automatic way, the inherent statements implied by the
hyperlinks can be made available on the Web. This realizes the
maximized reuse of the current Web (In most cases, the current
Web is rendered as Web of Documents) and provides a bridge
between the Web of Documents and the Web of Data. An
experimental study utilizing the resultant linked data for Small
Web search is introduced, in which the ontological information
about the webpages serves as the metadata and this provides the
potential to enhance the web search quality. The results of this
study show that, compared with the existing solutions, the
precision of Small Web search is significantly improved.","Linked Data, Semantic Web, Ontology, Metadata, Web Mining",H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
12/26/2020 20:25:11,2008,"SparqPlug: Generating Linked Data from Legacy HTML, SPARQL and the DOM",http://events.linkeddata.org/ldow2008/papers/05-coetzee-heath-sparqplug.pdf,Peter Coetzee,Imperial College London, United Kingdom,Tom Heath,Knights Court, United Kingdom,Enrico Motta,The Open University, United Kingdom,,,,,,,,,,,,,"The availability of linked RDF data remains a significant barrier to
the realisation of a Semantic Web. In this paper we present SparqPlug, an approach that uses the SPARQL query language and the
HTML Document Object Model to convert legacy HTML data sets
into RDF. This framework improves upon existing approaches in
a number of ways. For example, it allows the DOM to be queried
using the full flexibility of SPARQL and makes converted data automatically available in the Semantic Web. We outline the process
of batch conversion to RDF using SparqPlug and illustrate this with
a case study. The paper concludes with an examination of factors
affecting SparqPlug‚Äôs performance across various forms of HTML
data.","RDF, SPARQL, DOM, HTML, Conversion, Linked Data, Semantic Web, Data Mining",H.3.1 [Information Storage And Retrieval]: Content Analysis and Indexing
12/26/2020 20:27:45,2008,Building Linked Data For Both Humans and Machines,http://events.linkeddata.org/ldow2008/papers/06-halb-raimond-building-linked-data.pdf,Wolfgang Halb,Institute of Information Systems & Information Management, Austria,Yves Raimond,Centre for Digital Music, United Kingdom,Michael Hausenblas,Institute of Information Systems & Information Management, Austria,,,,,,,,,,,,,"In this paper we describe our experience with building the
riese dataset, an interlinked, RDF-based version of the Eurostat data, containing statistical data about the European
Union. The riese dataset (http://riese.joanneum.at), aims
at serving roughly 3 billion RDF triples, along with millions of high-quality interlinks. Our contribution is twofold:
Firstly, we suggest using RDFa as the main deployment
mechanism, hence serving both humans and machines to
effectively and efficiently explore and use the dataset. Secondly, we introduce a new way of enriching the dataset
with high-quality links: the User Contributed Interlinking,
a Wiki-style way of adding semantic links to data pages.","Linked data, Semantic Web, XHTML+RDFa, User Contributed Interlinking",H.4 [Information Systems Applications]: Miscellaneous 
12/26/2020 20:30:44,2008,Provenance and Linked Data in Biological Data Webs,http://events.linkeddata.org/ldow2008/papers/07-zhao-klyne-provenance-linked-data.pdf,Jun Zhao,Provenance and Linked Data in Biological Data Webs, United Kingdom,Graham Klyne,Provenance and Linked Data in Biological Data Webs, United Kingdom,David Shotton,Provenance and Linked Data in Biological Data Webs, United Kingdom,,,,,,,,,,,,,"To created a linked data web of heterogeneous biological
data resources, we need not only to define and create the
alignment between related data resources but also to express the knowledge about why data items from different
sources are linked with each other and how each data link
has evolved, so that scientists can trust the data links provided by the data web. This paper highlights the importance
of keeping provenance information about the links between
data items from different sources, and proposes the use of
named graphs to make a provenance statement about each
pair of linked data items and each release of a data web.
","Data Web, Named Graphs, Provenance, RDF, Semantic Web, Trust",D.2.12 [Software Engineering]: Interoperability‚ÄîData mapping; H.3.5 [Information Storage And Retrieval]: Online Information Services
12/26/2020 20:33:13,2008,"OPEN DATA COMMONS, A LICENSE FOR OPEN DATA",http://events.linkeddata.org/ldow2008/papers/08-miller-styles-open-data-commons.pdf,Paul Miller,Knights Court, United Kingdom,Rob Styles,Knights Court, United Kingdom,Tom Heath,Knights Court, United Kingdom,,,,,,,,,,,,,"Attendees at the WWW2007 panel session on Open Data [1, 2]
will remember a wide-ranging discussion of the role that easily
accessible data could play in endeavors from scholarly publishing
[3] to the creation of canonical product catalogs [4]. The authors
argued there and subsequently [5] that an effective and flexible
licensing framework is needed in moving forward.
Paradoxically, we argue that you need to actively and consciously
assert your desire that third parties be able to use data you place
online in order for those ‚Äòvisible‚Äô and ‚Äòaccessible‚Äô data sets to be
utilized most effectively.
Significant progress has been made in the past twelve months,
with engagement [6] from Creative Commons [7, 8, 9] and others
[10] resulting in a license [11] and notion of ‚Äòcommunity
norms‚Äô [12] upon which all can build.","Open Data, Linked Open Data, Licensing, License, Creative Commons, Science Commons, Open Data Commons.","E.m [Data, Miscellaneous]"
12/26/2020 20:35:25,2008,Browser-based Semantic Mapping Tool for Linked Data in Semantic Web,http://events.linkeddata.org/ldow2008/papers/09-zhou-xu-mapping-tool.pdf,Chunying Zhou,College of Computer Science, China,Chengli Xu,College of Computer Science, China,Huajun Chen,College of Computer Science, China,Kingsley Idehen,OpenLink Software Inc, US,,,,,,,,,,"With the ever growing need of semantically linking data across
different domain, organizational and diplomacy boundaries using
RDF and OWL, one of the major obstacles impeding the
advancement of Semantic Web is the data availability. This paper
presents a browser-based semantic mapping tool for converting
linked data on the Web to be available for Semantic Web
applications. It is aiming at helping users define semantic
mapping from relational databases to RDF schemas. This tool
provides much easy-to-use functionality to facilitate users in this
definition work, such as drag-and-drop mapping, visualization
mapping, data source annotation, database server independent,
standard mapping language and shared ontology management. ","Ontology, Semantic Mapping, Relational Database, RDF, Linked Data, Semantic Web.",H.4.m [Information Systems Applications]: Miscellaneous
12/26/2020 20:38:02,2008,:me owl:sameAs flickr:33669349@N00,http://events.linkeddata.org/ldow2008/papers/10-passant-me-owl-same-as.pdf,Alexandre Passant,Université Paris, France,,,,,,,,,,,,,,,,,,,"In order to release the Social Semantic Web and solve dataportability issues, there is a need from Web 2.0 providers
to open their content and deliver it in a machine-readable
way. Modeling it with vocabularies such as FOAF, SIOC,
as well as reusing data from the Linked Data initiative like
DBpedia or GeoNames can help to achieve this task. The
goal of this demo is to showcase an RDF exporter for Flickr
profiles that acts such a way.
","Web 2.0, Linked Data, Social Networks, Data Portability, Flickr, FOAF, SIOC",
12/26/2020 20:48:05,2008,Tabulator Redux: Browsing and Writing Linked Data,http://events.linkeddata.org/ldow2008/papers/11-berners-lee-hollenbach-tabulator-redux.pdf,T Berners-Lee,MIT, US,J. Hollenbach,University of Southampton, United Kingdom,Kanghao Lu,University of Southampton, United Kingdom,J. Presbrey,University of Southampton, United Kingdom,E. Prud'ommeaux,MIT, US,mc schraefel,MIT, US,,,,"A first category of Semantic Web browsers was designed to
present a given dataset (an RDF graph) for perusal in various
forms. These include mSpace, Exhibit, and to a certain extent
Haystack. A second category tackled mechanisms and display
issues around presenting linked data gathered on the fly. These
include Tabulator, Oink, Disco, Open Link Software's Data
Browser, and Object Browser. The challenge of once that data is
gathered, how might it be edited, extended and annotated has so
far been left largely unaddressed. This is not surprising: there are a
number of steep challenges for determining how to support
editing information in the open web of linked data. These include
the representation of both the web of documents and the web of
things, and the relationships between them; ensuring the user is
aware of and has control over the social context such as licensing
and privacy of data being entered, and, on a web in which anyone
can say anything about anything, helping the user intuitively
select the things which they actually wish to see in a given
situation. There is also the view update problem: the difficulty of
reflecting user edits back through functions used to map web data
to a screen presentation. In the latest version of the Tabulator
project, described in this paper we have focused on providing the
write side of the readable/writable web. Our approach has been to
allow modification and addition of information naturally within
the browsing interface, and to relay changes to the server triple by
triple for least possible brittleness (there is no explicit 'save'
operation). Challenges that remain include the propagation of
changes by collaborators back to the interface to create a shared
editing system. To support writing across (semantic) Web
resources, our work has contributed several technologies,
including a HTTP/SPARQL/Update-based protocol between an
editor (or other system) and incrementally editable resources
stored in an open source, world-writable 'data wiki'. This begins
enabling the writable Semantic Web.","Tabulator, semantic web, read/write, provenance",H.3.5 Online Information Services: Web Based Services
12/26/2020 20:54:43,2008,Searching Semantic Web Objects Based on Class Hierarchies,http://events.linkeddata.org/ldow2008/papers/12-cheng-ge-searching-semantic-web-objects.pdf,Gong Cheng,Southeast University, China,Weiyi Ge,Southeast University, China,Honghan Wu,Southeast University, China,Yuzhong Qu,Southeast University, China,,,,,,,,,,"On the Semantic Web, objects can be identified by URIs and
described by using RDF. With more and more RDF data
published, object search on the Semantic Web is in demand
for browsing, reuse, and integration. We developed the Falcons system, a keyword-based search engine for the Semantic
Web. In this paper, we present the object search service provided by Falcons. For building an inverted index from query
terms to Semantic Web objects (SW objects), we present
a method to construct comprehensive textual description of
SW objects. Furthermore, SW objects are also indexed from
their classes and ancestor classes to support the class-based
query restriction. Especially, class subsumption reasoning
on multiple vocabularies on the Semantic Web is performed,
and a class recommendation technique is proposed to enable
hierarchically navigating classes. In addition, a summarization method for SW objects is devised to enable users to
browse the summaries of objects","Class Hierarchy, Indexing, Navigation, Search Engine, Summarization",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
12/26/2020 20:56:42,2008,DBpedia Mobile: A Location-Enabled Linked Data Browser,http://events.linkeddata.org/ldow2008/papers/13-becker-bizer-dbpedia-mobile.pdf,Christian Becker,Freie Universitat Berlin, Germany,Christian Bizer,Freie Universitat Berlin, Germany,,,,,,,,,,,,,,,,"In this demonstration, we present DBpedia Mobile, a
location-centric DBpedia client application for mobile devices consisting of a map view and a Fresnel-based Linked
Data browser. The DBpedia project extracts structured
information from Wikipedia and publishes this information as Linked Data on the Web. The DBpedia dataset
contains information about 2.18 million things, including
almost 300,000 geographic locations. DBpedia is interlinked
with various other location-related datasets. Based on
the current GPS position of a mobile device, DBpedia
Mobile renders a map indicating nearby locations from the
DBpedia dataset. Starting from this map, users can explore
background information about locations and can navigate
into interlinked datasets. DBpedia Mobile demonstrates
that the DBpedia dataset can serve as a useful starting
point to ","Semantic Web, Linked Data, Geospatial Web, DBpedia, Location Based Applications",H.3.3 [Information Search and Retrieval]: Information filtering; H.5.4 [Hypertext/Hypermedia]: Navigation
12/26/2020 20:58:18,2008,Humboldt: Exploring Linked Data,http://events.linkeddata.org/ldow2008/papers/15-kobilarov-dickinson-humboldt-exploring.pdf,Georgi Kobilarov,Packard Labs, United Kingdom,Ian Dickinson,Packard Labs, United Kingdom,,,,,,,,,,,,,,,,"We present Humboldt, a novel user interface for browsing
RDF data. Current user interfaces for browsing RDF data
are reviewed. We argue that browsing tasks require both
a facet browser‚Äôs ability to select and process groups of resources at a time and a ‚Äôresource at a time‚Äô browser‚Äôs ability
to navigate anywhere in a dataset. We describe Humboldt
which combines these two features in a single coherent interface. Our approach is based on the operation of pivoting,
which enables the user to move the focus of a browsing from
one set of resources to a set of related resources. With repeated use of the pivot operation the user can browse anywhere in the data. We describe a preliminary evaluation of
our approach and discuss its implications for further development.","user interface, semantic web, faceted browsing, rdf, linked data",H5.2 [Information Interfaces and Presentation]: User Interfaces; H5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia
12/26/2020 21:00:01,2008,Browsing Linked Data with Fenfire,http://events.linkeddata.org/ldow2008/papers/14-hastrup-cyganiak-browsing-with-fenfire.pdf,Tuukka Hastrup,University of Jyväskylä, Finland,Richard Cyganiak,National University of Ireland, Ireland,Uldis Bojars,National University of Ireland, Ireland,,,,,,,,,,,,,"A wealth of information has recently become available as
browsable RDF data on the Web, but the selection of client
applications to interact with this Linked Data remains limited. We show how to browse Linked Data with Fenfire,
a Free and Open Source Software RDF browser and editor
that employs a graph view and focuses on an engaging and
interactive browsing experience. This sets Fenfire apart from
previous table- and outline-based Linked Data browsers.",,
12/26/2020 21:05:59,2008,zLinks: Semantic Framework for Invoking Contextual Linked Data,http://events.linkeddata.org/ldow2008/papers/16-bergman-giasson-zlinks-semantic-framework.pdf,Michael K. Bergman,Zitgist LLC, US,Frédérick Giasson,Zitgist LLC, Canada,,,,,,,,,,,,,,,,"This first-ever demonstration of the new zLinks plug-in shows
how any existing Web document link can be automatically
transformed into a portal to relevant Linked Data. Each existing
link disambiguates to its contextual and relevant subject concept
(SC) or named entity (NE). The SCs are grounded in the
OpenCyc knowledge base, supplemented by aliases and WordNet
synsets to aid disambiguation. The NEs are drawn from
Wikipedia as processed via YAGO, and other online fact-based
repositories. The UMBEL ontology basis to this framework
offers significant further advantages. The zLinks popup is
invoked only as desired via unobtrusive user interface cues. ","demo, Linked Data, zLinks, OpenCyc, Wikipedia, WordNet, YAGO, UMBEL. ","H.3.3 [Information Search and Retrieval]: Information filtering, Query formulation; H.5.4 [Hypertext/Hypermedia]: Navigation; H.5.2 [User Interfaces]: Interaction styles. "
12/26/2020 21:19:03,2008,n2Mate: Exploiting social capital to create a standards-rich semantic network,http://events.linkeddata.org/ldow2008/papers/17-peterson-cregan-n2mate-semantic-network.pdf,David Peterson,BoaB interactive, Australia,Anne Cregan,National ICT Australia, Australia,Rob Atkinson,CSIRO Land & Water Lucas Heights Research Laboratories, Australia,John Brisbin,BoaB interactive, Australia,,,,,,,,,,"A significant boost on the path towards a web of linked, open data
is the establishment and promotion of common semantic resources
including ontologies and other operationalised vocabularies, and
their instance data. Without consensus on these, we are
hamstrung by the famous ‚Äún-squared‚Äù mapping problem. In
addition, each vocabulary has its own associated attributes to do
with why it was developed, what purposes it is best suited for, and
how accurate and reliable it is at both a content and technical
level, but most of this information is opaque to the general
community.
Our theory is that it is the lack of socially-sensitised processes
highlighting who is using what and why, that have led to the
current unmanageable plethora of vocabularies, where it is far
easier to build your own vocabulary than try to find a suitable,
reliable existing one.
We therefore suggest that there is considerable value in the
development of an online facility that performs the function of
providing a space listing vocabulary and ontology resources with
their associated authority, governance and quality of service
attributes. Presenting this in a visual form and providing pivotable
search facilities enhances recognition and comprehension.
Additionally, and critically, the facility provides a focal point
where discourse communities can make authority claims, rate
vocabularies on various parameters, register their commitment to
or usage of particular vocabularies, and provide feedback on their
experiences. Through social interaction, we expect the most solid
and useful vocabularies to emerge and form a stable semantic
platform for content representation and interlinked knowledge.
Our strategy is to become sufficiently enmeshed in the native
information habits of people and their derivative institutions to
reveal and collect their standards-seeking needs and activities with
a minimum of effort on their part.
This paper describes a pilot facility testing the theory above.
Dubbed ‚Äún2Mate‚Äù, it is a novel exploitation of social networking
software to provide a lightweight and flexible platform for testing
the efficacy of leveraging social networks to link existing registers
and ‚Äûseed‚Äü an information space focussing on the use of standards
in online information management.
The paper uses examples from the Australian context to provide
clear illustration of the central arguments.
","Registers, vocabularies, standards, linking density, rdf graph, social networking, knowledge re-use, n2Mate, n-squared",
12/26/2020 21:31:18,2008,Automatic Interlinking of Music Datasets on the Semantic Web,http://events.linkeddata.org/ldow2008/papers/18-raimond-sutton-automatic-interlinking.pdf,Yves Raimond,"Queen Mary, University of London", United Kingdom,Christopher Sutton,"Queen Mary, University of London", United Kingdom,Mark Sandler,"Queen Mary, University of London", United Kingdom,,,,,,,,,,,,,"In this paper, we describe current efforts towards interlinking music-related datasets on the Web. We first explain
some initial interlinking experiences, and the poor results
obtained by taking a na¬®ƒ±ve approach. We then detail a particular interlinking algorithm, taking into account both the
similarities of web resources and of their neighbours. We
detail the application of this algorithm in two contexts: to
link a Creative Commons music dataset to an editorial one,
and to link a personal music collection to corresponding web
identifiers. The latter provides a user with personally meaningful entry points for exploring the web of data, and we
conclude by describing some concrete tools built to generate
and use such links.","Semantic-Web,Linked Data,Music",H.4 [Information Systems Applications]: Miscellaneous
12/26/2020 21:59:20,2008,URI Disambiguation in the Context of Linked Data ,http://events.linkeddata.org/ldow2008/papers/19-jaffri-glaser-uri-disambiguation.pdf,Afraz Jaffri,University of Southampton, United Kingdom,Hugh Glaser,University of Southampton, United Kingdom,Ian Millard,University of Southampton, United Kingdom,,,,,,,,,,,,,"The Linked Data initiative has given rise to an increasing number
of RDF datasets, many of which are freely accessible online.
These resources often arise as a result of database exports;
however sufficient consideration may not be given to the unseen
implications caused when they are used in the wider context of the
Semantic Web. This paper investigates two popular resources,
DBLP and DBpedia, and discusses whether the issues regarding
identity management and co-reference resolution have been
suitably addressed. We find that a large percentage of authors in
DBLP have been conflated, and that disambiguation pages have
been incorrectly linked using owl:sameAs within DBpedia.
Systems for dealing with these issues are presented, and directions
are given for future research. 
","Linked Data, URI, Co-reference ","H.3.5 [Information Systems]: Information Storage and Retrieval: Online Information Services ‚Äì data sharing, web-based services. "
12/26/2020 22:01:29,2008,Linked Data Spaces & Data Portability,http://events.linkeddata.org/ldow2008/papers/20-idehen-erling-linked-data-spaces.pdf,Kingsley Idehen,OpenLink Software Inc, US,Orri Erling,OpenLink Software Inc, US,,,,,,,,,,,,,,,,"In the year 2007, the size of the Linked Data injected into the Web
grew to several billion RDF triples, served by a network of
interlinked data sources that cover domains such as general
knowledge, geographic information, people, companies, online
communities, films, music, books and scientific publications.
Unfortunately, the growth rate of User Generated content from a
variety of Web based unstructured and semi-structured data-silos
continues to exceed that of structured Linked Data. Thus, we have
a pressing need for technology, capable of bridging this
broadening divide via transparent generation of Linked Data from
existing data-silos on the Web. Our Linked Data technology
demonstration explores the use of the OpenLink Data Spaces
platform as a solution to this problem.","Linked Data, Semantic Web, SPARQL, Data Integration, Data Spaces",H.3.2 [Information Storage] H.3.3 [Information Search & Retrieval]
12/26/2020 22:04:11,2008,Linking Enterprise Data,http://events.linkeddata.org/ldow2008/papers/21-servant-linking-enterprise-data.pdf,François-Paul Servant,Renault, France,,,,,,,,,,,,,,,,,,,"The ‚ÄúLinking Open Data‚Äù community initiative contributed a
great deal to the concretization of the web of data, describing best
practice, publishing large sets of RDF data on the web, and
consequently giving birth to a new area of possibilities for
innovative mashups using these data. Enterprises‚Äô information
systems too can be envisioned as a space of linked data. We
describe herein how we used Linked Data principles in a work
intended to foster adoption of semantic web technologies in our
company.","RDF in the enterprise, RDF based web services, Linking Enterprise Data, Linked Data",H.4.m [Information Systems]: Miscellaneous; D.2 [Software]: Software Engineering
12/26/2020 22:06:15,2008,Meaning Of A Tag: A Collaborative Approach to Bridge the Gap Between Tagging and Linked Data,http://events.linkeddata.org/ldow2008/papers/22-passant-laublet-meaning-of-a-tag.pdf,Alexandre Passant,Electricité de France, France,Philippe Laublet,Université Paris, France,,Université Paris, France,,,,,,,,,,,,,"This paper introduces MOAT, a lightweight Semantic Web
framework that provides a collaborative way to let Web 2.0
content producers give meanings to their tags in a machinereadable way. To achieve this goal, this approach relies on
Linked Data principles, using URIs from existing resources
to define these meanings. That way, users can create interlinked RDF data and let their content enter the Semantic
Web, while solving some limits of free-tagging at the same
time.","Semantic Web, Web 2.0, Tagging, MOAT, Linked Data, Architecture of Participation, SIOC",
12/26/2020 22:08:50,2008,An Entity Name System for Linking Semantic Web Data,http://events.linkeddata.org/ldow2008/papers/23-bouquet-stoermer-entity-name-system.pdf,Paolo Bouquet,University of Trento, Italy,Heiko Stoermer,University of Trento, Italy,Daniele Cordioli,National University of Ireland, Ireland,,ExpertSystem s.p.a., Italy,,,,,,,,,,"In this paper, we argue that realizing the vision of the Semantic Web as an open and global decentralized knowledge space would greatly benefit from the availability of a
large-scale service which may enable the systematic reuse
of URIs across independently produced Semantic Web contents. Such a service, which we call an Entity Name System
(ENS), might play for the Semantic Web the role that the
DNS played for interlinking hypertexts on the Web. The key
idea behing the ENS is that any tool/application for creating content (not only Semantic Web) may be empowered
through simple plugins or extensions to interact with a network of ENS servers. Here we show how this has been done
in a few prototypes for MS Word, Prot¬¥eg¬¥e and an editor for
FOAF profiles","Semantic Web, Entity Name System, Identity",H.4 [Information Systems Applications]: Miscellaneous
12/26/2020 22:10:56,2009,Representing Linked Data as Virtual File Systems,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper1.pdf,Bernhard Schandl,University of Vienna, Austria,,,,,,,,,,,,,,,,,,,"One of the main characteristics of Linked Open Data (LOD)
is the exclusive application of standards published and maintained by the World Wide Web Consortium. This strict
adherence is kept on all levels, ranging from the identification and transportation (URI, HTTP) to the interpretation (RDF, RDFS, OWL) of resource descriptions. Because these standards are open and accessible to everybody,
broad acceptance and proliferation of LOD technologies in
Web-based applications and services are enabled. On typical desktops, however, the majority of applications are not
aware of Web standards, but use hierarchical file systems to
organize and store information. This results in a gap between the two distinct information spaces of the Web and
the desktop. To bridge this gap, we propose a virtual file
system representation of LOD sets, through which they can
be accessed as if they were present in the file system and
thus easily be used within desktop applications.","Linked Open Data, file systems, information representation, Semantic Desktop",D.4.3 [Operating Systems]: File Systems Management; H.3.5 [Information Storage and Retrieval]: Online Information Services
12/26/2020 22:12:15,2009,Explorator: a tool for exploring RDF data through direct manipulation.,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper2.pdf,Samur F. C.,Catholic University of Rio de Janeiro, Brazil,Daniel Schwabe,Catholic University of Rio de Janeiro, Brazil,,,,,,,,,,,,,,,,"In this paper we introduce Explorator, a tool for exploring the
Semantic Web data by direct manipulation. Explorator
implements a model of operations that is supported by a visual
interface that enables the user, with minimal knowledge of RDF
model, to explore an RDF database without a-priori knowledge of
data domain. Consequently, it is well suited for tasks that involve
information search, exploration and visualization.","RDF, exploratory search, exploration, ontology, semantic web.","H.5.3 Web-based interaction; H.5.4 Hypertext/Hypermedia - Navigation, H.3.3 Information Search and Retrieval ‚Äì search process, query formulation."
12/26/2020 22:54:30,2009,Faceted Views over Large-Scale Linked Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper3.pdf,Orri Erling,OpenLink Software Inc, US,,,,,,,,,,,,,,,,,,,"Faceted views over structured and semi structured data have
been popular in user interfaces for some years. Deploying such views of arbitrary linked data at arbitrary scale
has been hampered by lack of suitable back end technology. Many ontologies are also quite large, with hundreds of
thousands of classes.
Also, the linked data community has been concerned with
the processing cost and potential for denial of service presented by public SPARQL end points.
This paper discusses how we use Virtuoso Cluster Edition
for providing interactive browsing over billions of triples,
combining full text search, structured querying and result
ranking. We discuss query planning, run time inferencing
and partial query evaluation. This functionality is exposed
through SPARQL, a specialized web service and a web user
interface",H.5.4 [Information Systems]: Hypertext/Hypermedia; H.2.8 [Information Systems]: Database Applications,"Keywords Faceted Views, Linked Data, SPARQL, OpenLink Virtuoso, partial query evaluation, entity ranking, large ontologies"
12/26/2020 22:55:50,2009,Linked Data Authoring for Non-Experts,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper4.pdf,Markus Luczak-Rosch,Freie Universitat Berlin, Germany,Ralf Heese,Freie Universitat Berlin, Germany,,,,,,,,,,,,,,,,"The vision of the Semantic Web community is to create a
linked ‚ÄúWeb of data‚Äù providing ubiquitous data access via
machine understandable links between data resources. Due
to the need of enormous expertise for producing and consuming linked data, the idea of linked data emerges only
slowly and only a few data sources are currently available as
linked data. In this paper we present Loomp to facilitate an
increasing use of the Web data. Loomp enables non-experts
to produce and publish semantically annotated content as
easy as formatting text in word processors. Furthermore,
Loomp simplifies the reuse of content with different Web
applications.
","linked data, content authoring and publishing, Web of data","I.7.4 [Electronic Publishing]; H.4.1 [Information Systems Applications]: Office Automation‚ÄîDesktop publishing,Word processing; H.3.2 [Information Storage and Retrieval]: Information Storage"
12/26/2020 22:57:30,2009,Linking and Navigating Data in a P2P File-Sharing Network,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper10.pdf,Alan Davoust,Carleton University, Canada,Babak Esfandiari,Carleton University, Canada,,,,,,,,,,,,,,,,"We demonstrate a tool for publishing and navigating linked
data over the highly dynamic infrastructure of a P2P filesharing network. Our links are based on a URI scheme which
allows unambiguous designation of replicated data items, regardless of their location in the network. In a true decentralized P2P spirit, users publish and distribute the links
just like other data items.
","Peer-to-peer, linked data, URI, demo",H.5.4 [Hypertext/Hypermedia]: Navigation
12/26/2020 22:58:49,2009,Interlinking Distributed Social Graphs,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper5.pdf,Matthew Rowe,University of Sheffield, United Kingdom,,,,,,,,,,,,,,,,,,,"The rise in use of the social web has forced web users to
duplicate their identity in fragmented information spaces.
Commonly these spaces contain rich identity representations
hidden within walled garden data silos. This paper presents
work to export social graphs from such data silos as RDF
datasets, and provide linkage between these social graphs
according to a graph matching paradigm. Our work contributes to the linked data movement by providing a decentralised social graph containing linked data describing
fragmented identity components.
","Semantic Web, Social Web, Linked Data, RDF, FOAF",H.4 [Information Systems Applications]: General
12/26/2020 23:00:34,2009,Publishing XBRL as Linked Open Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper6.pdf,Roberto García,Universitat de Lleida , Spain,Rosa Gil,Universitat de Lleida , Spain,,,,,,,,,,,,,,,,"The XML Business Reporting Language (XBRL) is a standard for
business and financial information reporting. It is based on XML
so instance documents based on XBRL, e.g. a quarterly report, are
highly constrained by the XML document-oriented nature. This
makes more difficult to perform queries that mix information
from filings from different dates, companies, or accounting
principles than with a formalism based on a graph model instead
of a tree model. Semantic Web technologies provide a graph
model that facilitates mashing-up different XBRL sources. We
have put into practice this approach mapping the XBRL filings
available from the SEC‚Äôs EDGAR program to Resource
Description Framework (RDF) and the XML Schema taxonomies
these filings are based on to Web Ontology Language (OWL).
The resulting semantic metadata, though highly tied to the XML
structure it is mapped from, benefits from Semantic Web
technologies and tools in order to facilitate integration and crossquerying, even together with other parts of the Web of Linked
Data.","Business, reporting, Semantic Web, Linked Data, Web 3.0, accounting, finance, interoperability.",
12/26/2020 23:02:18,2009,Bringing the Thesaurus for Economics on to the Web of Linked Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper7.pdf,Joachim Neubert,Leibniz Information Centre for Economics, Germany,,,,,,,,,,,,,,,,,,,"Thesauri are possible building blocks of a web of linked data. As
DBpedia for large data sets in general, specialized thesauri could
be useful as interlinking hubs for professional communities ‚Äì if
they are available on the linked data web. The paper describes the
conversion of a large economics thesaurus to RDF/SKOS, using
the enhancement mechanisms of SKOS to dispose some nonstandard features of this thesaurus. The deployment, using RDFa
pages, and the interlinking with other resources, namely a library
catalog, and an experimental mapping to DBpedia are presented.
For information retrieval support, a SPARQL query facility uses
the data for building a thesaurus-backed terminology web service.","Thesaurus, SKOS, RDFa, Web Service, REST, Library, Catalog, Economics",H.3.1 [Content Analysis and Indexing] Thesauruses; I.2.4 [Knowledge Representation Formalisms and Methods] Semantic networks; H.3.7 [Digital Libraries] Standards
12/26/2020 23:06:18,2009,Adding eScience Assets to the Data Web,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper8.pdf,Herbert Van de Sompel,Los Alamos National Laboratory, US,Carl Lagoze,Cornell University, US,Michael L. Nelson,Old Dominion University, US,Simeon Warner,Cornell University, US,Robert Sanderson,University of Liverpool, United Kingdom,Pete Johnston,Eduserv Foundation, United Kingdom,,,,"Aggregations of Web resources are increasingly important in
scholarship as it adopts new methods that are data-centric,
collaborative, and networked-based. The same notion of aggregations of resources is common to the mashed-up, socially
networked information environment of Web 2.0. We present
a mechanism to identify and describe aggregations of Web
resources that has resulted from the Open Archives Initiative - Object Reuse and Exchange (OAI-ORE) project. The
OAI-ORE specifications are based on the principles of the
Architecture of the World Wide Web, the Semantic Web,
and the Linked Data effort. Therefore, their incorporation
into the cyberinfrastructure that supports eScholarship will
ensure the integration of the products of scholarly research
into the Data Web.","Cyberinfrastructure, eScience, OAI-ORE, Web Architecture, Linked Data, RDF, Atom",H.5.4 [Information Systems]: Hypertext/Hypermediav
12/26/2020 23:09:04,2009,Enabling Tailored Therapeutics with Linked Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper9.pdf,Anja Jentzsch,Freie Universität Berlin, Germany,Bo Andersson,AstraZeneca R&D Lund , Sweden,Oktie Hassanzadeh,University of Toronto, Canada,Susie Stephens,Eli Lilly and Company, US,Christian Bizer,Freie Universität Berlin, Germany,,,,,,,"Advances in the biological sciences are allowing pharmaceutical
companies to meet the health care crisis with drugs that are more
suitable for preventive and tailored treatment, thereby holding the
promise of enabling more cost effective care with greater efficacy
and reduced side effects. However, this shift in business model
increases the need for companies to integrate data across drug
discovery, drug development, and clinical practice. This is a
fundamental shift from the approach of limiting integration
activities to functional areas. The Linked Data approach holds
much potential for enabling such connectivity between data silos,
thereby enabling pharmaceutical companies to meet the urgent
needs in society for more tailored health care. This paper
examines the applicability and potential benefits of using Linked
Data to connect drug and clinical trials related data sources and
gives an overview of ongoing work within the W3C's Semantic
Web for Health Care and Life Sciences Interest Group on
publishing drug related data sets on the Web and interlinking
them with existing Linked Data sources. A use case is provided
that demonstrates the immediate benefit of this work in enabling
data to be browsed from disease, to clinical trials, drugs, targets
and companies.","Linked Data, Semantic Web, Tailored Therapeutics, Drugs, Clinical Trials, Competitive Intelligence",H.3.5 [Online Information Services]: Data Sharing
12/26/2020 23:27:06,2009,Managing Co-reference on the Semantic Web,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper11.pdf,Hugh Glaser,University of Southampton, United Kingdom,Afraz Jaffri,University of Southampton, United Kingdom,Ian C. Millard,University of Southampton, United Kingdom,,,,,,,,,,,,,"Co-reference resolution, or the determination of ‚Äòequivalent‚Äô
URIs referring to the same concept or entity, is a significant
hurdle to overcome in the realisation of large scale Semantic Web applications. However, it has only recently gained
the attention of research communities in the Semantic Web
context, and while activities are now underway in identifying
co-referent or conflated URIs, little consideration has been
given to tools and techniques for storing, manipulating, and
reusing co-reference information.
This paper provides an overview of the specification, implementation, interactions and experiences in using the Coreference Resolution Service (CRS) to facilitate rigorous management of URI co-reference data, and enable interoperation
between multiple Linked Open Data sources. Comparisons
are made throughout the paper contrasting the differences
in the way the CRS manages multiple URIs for the same
resource with the emerging practice of using owl:sameAs to
identify duplicate URIs. The advantages and benefits that
have been gained from deploying the CRS on a site with
multiple Linked Data repositories are also highlighted.","Co-reference, Linked Data, Semantic Web","H.3.5 [Information Systems]: Information storage and retrieval‚ÄîOnline Information Services ‚Äì data sharing, web based services"
12/26/2020 23:29:19,2009,Linked Movie Data Base,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper12.pdf,Oktie Hassanzadeh,University of Toronto, Canada,Mariano Consens,University of Toronto, Canada,,,,,,,,,,,,,,,,"The Linked Movie Database (LinkedMDB) project provides
a demonstration of the first open linked dataset connecting several major existing (and highly popular) movie web
resources. The database exposed by LinkedMDB contains
millions of RDF triples with hundreds of thousands of RDF
links to existing web data sources that are part of the growing Linking Open Data cloud, as well as to popular movie related web pages such as IMDb. LinkedMDB uses a novel
way of creating and maintaining large quantities of high
quality links by employing state-of-the-art approximate join
techniques for finding links, and providing additional RDF
metadata about the quality of the links and the techniques
used for deriving them.","Linked Data, Semantic Web, Semantic Link Discovery, Record Linkage, Movie Database",
12/26/2020 23:33:24,2009,Silk- A Link Discovery Framework for the Web of Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper13.pdf,Julius Volz,Chemnitz University of Technology, Germany,Christian Bizer,Freie Universität Berlin, Germany,Martin Gaedke,Chemnitz University of Technology, Germany,Georgi Kobilarov,Freie Universität Berlin, Germany,,,,,,,,,,"The Web of Data is built upon two simple ideas: Employ the RDF
data model to publish structured data on the Web and to set
explicit RDF links between entities within different data sources.
This paper presents the Silk ‚Äì Link Discovery Framework, a tool
for finding relationships between entities within different data
sources. Data publishers can use Silk to set RDF links from their
data sources to other data sources on the Web. Silk features a
declarative language for specifying which types of RDF links
should be discovered between data sources as well as which
conditions entities must fulfill in order to be interlinked. Link
conditions may be based on various similarity metrics and can
take the graph around entities into account, which is addressed
using a path-based selector language. Silk accesses data sources
over the SPARQL protocol and can thus be used without having
to replicate datasets locally.","Linked data, link discovery, record linkage, similarity, RDF",H.2.3 [Database Management]: Languages 
12/26/2020 23:35:01,2009, Data Mashup Language for the Data Web ,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper14.pdf,Mustafa Jarrar,University of Cyprus, Cyprus,Marios D. Dikaiakos,University of Cyprus, Cyprus,,,,,,,,,,,,,,,,"This paper is motivated by the massively increasing structured
data on the Web (Data Web), and the need for novel methods to
exploit these data to their full potential. Building on the
remarkable success of Web 2.0 mashups, this paper regards the
internet as a database, where each web data source is seen as a
table, and a mashup is seen as a query over these sources. We
propose a data mashup language, which allows people to
intuitively query and mash up structured and linked data on the
web. Unlike existing query methods, the novelty of MashQL is
that it allows people to navigate, query, and mash up a data
source(s) without any prior knowledge about its schema,
vocabulary, or technical details. We even do not assume even that
a data source should an online or inline schema. Furthermore,
MashQL supports query pipes as a built-in concept, rather than
only a visualization of links between modules. ",,
12/26/2020 23:36:57,2009,Towards Data Fusion in a Multi-ontology Environment,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper15.pdf,Andriy Nikolov,Open University, United Kingdom,Victoria Uren,Open University, United Kingdom,Enrico Motta,Open University, United Kingdom,,,,,,,,,,,,,"With the growing amount of semantic data being published
on the Web the problem of finding individuals in different
datasets which correspond to the same entity is gaining importance. Given that datasets are often structured using
different ontologies, automatic schema-matching techniques
have to be utilized before proceeding with data-level alignment. In this paper we discuss how ontology schema mismatches influence data-level alignment based on our first
experience with implementing a data fusion tool for a multiontology environment.","Data fusion, coreference resolution, linked data",H.4.m [Information Systems]: Miscellaneous; D.2 [Software]: Software Engineering
12/27/2020 10:26:03,2009,A Query-Driven Characterization of Linked Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper16.pdf,Harry Halpin,University of Edinburgh, United Kingdom,,,,,,,,,,,,,,,,,,,"Due to the Linked Data initiative, the once unpopulated Semantic Web is now rapidly being populated with millions
of facts stored in RDF. Could any of this data possibly be
interesting to ordinary users? In this study, we run queries
extracted from a query log from a major hypertext search
engine against a Semantic Web search engine to determine
if the Semantic Web has anything of interest to the average Web user. There is indeed much Semantic Web information that could be relevant for many queries for entities (like people and places) and abstract concepts, although
these possibly relevant results are overwhelmingly clustered
around DBPedia. We present an empirical analysis of the
results, focusing on their major sources, the structure of the
triples, the use of various RDF and OWL constructs, and
the power-law distributions produced by both the URIs that
serve Linked Data and the URIs in the triples themselves.
The issue of 303 re","Linked Data statistics, query logs, information retrieval,power law",H.3.d [Information Technology and Systems]: Metadata
12/27/2020 10:30:46,2009,Interlinking Multimedia how to apply linked data principles to multimedia fragments,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper17.pdf,Michael Hausenblas,National University of Ireland, Ireland,Raphaël Troncy,CWI Amsterdam, The Netherlands,Tobias Bürger,STI Innsbruck, Austria,Yves Raimond,BBC Audio & Music interactive, United Kingdom,,,,,,,,,,"In this paper, we introduce interlinking multimedia (iM), a
pragmatic way to apply the linked data principles to fragments of multimedia items. We report on use cases showing
the need for retrieving and describing multimedia fragments.
We then introduce the principles for interlinking multimedia in the Web of Data, discussing potential solutions which
sometimes highlight controversial debates regarding what
the various representations of a Web resource span. We
finally present methods for enabling a widespread use of interlinking multimedia.
","Linked Data, Media Fragments, Media Annotations","H.4.m [Information Systems]: Miscellaneous; I.7.2 [Document Preparation]: Languages and systems, Markup languages, Multi/mixed media, Standards; I.2.4 [Knowledge Representation Formalisms and Methods]: Representation languages"
12/27/2020 10:32:35,2009,Provenance Information in the Web of Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper18.pdf,Olaf Hartig,Universitat zu Berlin, Germany,,,,,,,,,,,,,,,,,,,"The openness of the Web and the ease to combine linked
data from different sources creates new challenges. Systems
that consume linked data must evaluate quality and trustworthiness of the data. A common approach for data quality
assessment is the analysis of provenance information. For
this reason, this paper discusses provenance of data on the
Web and proposes a suitable provenance model. While traditional provenance research usually addresses the creation
of data, our provenance model also represents data access,
a dimension of provenance that is particularly relevant in
the context of Web data. Based on our model we identify
options to obtain provenance information and we raise open
questions concerning the publication of provenance-related
metadata for linked data on the Web.
","Provenance, Lineage, Web Data, Web of Data, Linked Data",I.2.4 [Computing Methodologies]: Knowledge Representation Formalisms and Methods; H.3.3 [Information Systems]: Information Search and Retrieval
12/27/2020 10:36:40,2009,An Ontology of Resources for Linked Data,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper19.pdf,Harry Halpin,University of Edinburgh, United Kingdom,Valentina Presutti,Semantic Technology Laboratory, Italy,,,,,,,,,,,,,,,,"The primary goal of the Semantic Web is to use URIs as
a universal space to name anything, expanding from using
URIs for webpages to URIs for ‚Äúreal objects and imaginary concepts,‚Äù as phrased by Berners-Lee. This distinction has often been tied to the distinction between information resources, like webpages and multimedia files, and
non-information resources, which are everything from real
people to abstract concepts like ‚Äòthe integers.‚Äô Furthermore,
the W3C has recommended not to use the same URI for
information resources and non-information resources, and
several communities like the Linked Data initiative are deploying this principle. The definition put forward by the
W3C, that information resources are things whose ‚Äúessential nature is information‚Äù is a difficult distinction at best.
For example, would the text of Moby Dick be an information
resource? While this problem could safely be ignored up until recently, with the rise of Linked Data and projects like
OKKAM, it appears that this problem should be modelled
formally. An ontology called IRW (Identity and Reference
on the Web) of various types of resources and their relationships, both for the hypertext Web and Linked Data, is
presented. It builds upon Information Object Lite (an extension of DOLCE Ultra Lite for describing information objects) and IRE (an earlier ontology of and aligns with other
work in this area. This ontology can be used as a tool to
make Linked Data more self-describing and to allow inference to be used to test for membership in various classes of
resources.","Linked Data, ontology, resource, Web architecture",H.3.d [Information Technology and Systems]: Metadata
12/27/2020 10:42:22,2009,"Describing Linked Datasets on the design and usage of void , the vocabulary of interlinked datasets",http://events.linkeddata.org/ldow2009/papers/ldow2009_paper20.pdf,Keith Alexander,Talis Ltd., United Kingdom,Richard Cyganiak,National University of Ireland, Ireland,Michael Hausenblas,National University of Ireland, Ireland,Jun Zhao,University of Oxford, United Kingdom,,,,,,,,,,"In this paper we discuss the design and implementation of
voiD, the ‚ÄúVocabulary Of Interlinked Datasets‚Äù, a vocabulary that allows to formally describe linked RDF datasets.
We report on use cases for voiD, the current state of the
specification and its potential applications in the context of
linked datasets.",,
12/27/2020 10:47:50,2009,DING! Dataset Ranking using Formal Descriptions,http://events.linkeddata.org/ldow2009/papers/ldow2009_paper21.pdf,Nickolai Toupikov,National University of Ireland, Ireland,Jurgen Umbrich,National University of Ireland, Ireland,Renaud Delbru,National University of Ireland, Ireland,Michael Hausenblas,National University of Ireland, Ireland,Giovanni Tummarello,National University of Ireland, Ireland,,,,,,,"Considering that thousands if not millions of linked datasets
will be published soon, we motivate in this paper the need
for an efficient and effective way to rank interlinked datasets
based on formal descriptions of their characteristics. We
propose DING (from Dataset RankING) as a new approach
to rank linked datasets using information provided by the
voiD vocabulary. DING is a domain-independent link analysis that measures the popularity of datasets by considering
the cardinality and types of the relationships. We propose
also a methodology to automatically assign weights to link
types. We evaluate the proposed ranking algorithm against
other well known ones, such as PageRank or HITS, using
synthetic voiD descriptions. Early results show that DING
performs better than the standard Web ranking algorithms.",,
12/27/2020 11:35:57,2010,Data.dcs: Converting Legacy Data into Linked Data,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper01.pdf,Matthew Rowe,University of Sheffield, United Kingdom,,,,,,,,,,,,,,,,,,,"Data.dcs is a project intended to produce Linked Data describing the University of Sheffield‚Äôs Department of Computer Science. At present the department‚Äôs web site contains important legacy data describing people, publications
and research groups. This data is distributed and is provided in heterogeneous formats (e.g. HTML documents,
RSS feeds), making it hard for machines to make sense
of such data and query it. This paper presents an approach to convert such legacy data from its current form
into a machine-readable representation which is linked into
the Web of Linked Data. The approach describes the triplification of legacy data, coreference resolution and interlinking
with external linked datasets.","Linked Data, Triplification, Coreference Resolution",H.4 [Information Systems Applications]: General
12/27/2020 11:51:47,2010,Lifting File Systems into the Linked Data Cloud with TripFS,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper02.pdf,Bernhard Schandl,University of Vienna, Austria,Niko Popitsch,University of Vienna, Austria,,,,,,,,,,,,,,,,"A major fraction of digital information is stored in file systems. File systems organize files usually in labelled directory
trees and provide a minimum support for user-driven file annotation, linkage and categorization. Although file systems
play a major role in knowledge organization, both in enterprise contexts as well as in the personal information sphere,
they have rarely been considered in Web-based information
integration. To a large extent, this can be contributed to the
limited metadata support of file systems and to the lack of
stable identifiers for file and directories, which makes it hard
to expose these objects in a global Web. We present TripFS,
a lightweight approach for exposing parts of local filesystems
as Linked Data. Serving file system objects via dereferenceable HTTP URIs paves the way to integrate them with the
Web of Data, and enables new possibilities of exploiting file
system data, for example, by linking them with other data
sources or by annotating them using Semantic Web technologies.","Linked Data, file systems, file metadata, information representation, information integration, event detection   ",D.4.3 [Operating Systems]: File Systems Management; H.3.5 [Information Storage and Retrieval]: Online Information Services
12/27/2020 12:39:00,2010,Semantic Statistics: Bringing Together SDMX and SCOVO,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper03.pdf,Richard Cyganiak,Digital Enterprise Research Institute, Ireland,Simon Field,Office for National Statistics, United Kingdom,Arofan Gregory,metadata technology,US,Wolfgang Halb,JOANNEUM RESEARCH, Austria,Jeni Tennison,The Stationery Office, United Kingdom,,,,,,,"Whether it's population, income, unemployment or interest rates,
statistical data is a fundamental source of information for analysis
and visualisations. Many publishers of statistics use SDMX to
represent statistics and make them available through web services.
The linked data principles of identifying items with HTTP URIs
and representing data using RDF provide some benefits (though
also some costs) for statistical publishing. This paper describes
how the SDMX information model can be used with linked data
and RDF and describes some ongoing work to explore the impact
of doing so.","statistics, linked data, RDF, SDMX, SCOVO, open data",H.4 [Information Systems]: Information Systems Applications
12/27/2020 12:51:13,2010,Weaving the Pedantic Web,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper04.pdf,Aidan Hogan,National University of Ireland, Ireland,Andreas Harth,National University of Ireland, Ireland,Alexandre Passant,National University of Ireland, Ireland,Stefan Decker,National University of Ireland, Ireland,Axel Polleres,National University of Ireland, Ireland,,,,,,,"Over a decade after RDF has been published as a W3C recommendation, publishing open and machine-readable content on the Web has recently received a lot more attention,
including from corporate and governmental bodies; notably
thanks to the Linked Open Data community, there now exists a rich vein of heterogeneous RDF data published on the
Web (the so-called \Web of Data"") accessible to all. However, RDF publishers are prone to making errors which compromise the eectiveness of applications leveraging the resulting data. In this paper, we discuss common errors in
RDF publishing, their consequences for applications, along
with possible publisher-oriented approaches to improve the
quality of structured, machine-readable and open data on
the Web.",,
12/27/2020 20:46:34,2010,openChart: Charting Quantitative Properties in LOD,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper06.pdf,Filip Zembowicz,Harvard University, US,David Opolon,MIT ESD, US,Stephen Miles,MIT AutoID Labs, US,,,,,,,,,,,,,"In this paper, we discuss the development of openChart, a
quantitative Linked Open Data charting tool. It targets novice
semantic web users by generating SPARQL queries to present
interesting information. We also acknowledge the problems
encountered in development and suggest improvements.","Linked Open Data, Visualization, Charting ","H.3.4 [Semantic Web]: Visualization, charting, search. "
12/27/2020 20:47:44,2010,Exploring RDF Usage and Interlinking in the Linked Open Data Cloud using ExpLOD,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper05.pdf,Shahan Khatchadourian,University of Toronto, Canada,Mariano P. Consens,University of Toronto, Canada,,,,,,,,,,,,,,,,"Publishing interlinked RDF datasets as links between data items
identified using dereferenceable URIs on the web brings forward
a number of issues. A key challenge is to understand the data,
the schema, and the interlinks that are actually used both within
and across linked datasets. Understanding actual RDF usage is
critical in the increasingly common situations where terms from
different vocabularies are mixed.
In this demonstration we present a tool, ExpLOD, that supports exploring summaries of RDF usage and interlinking among
datasets from the Linked Open Data cloud. ExpLOD‚Äôs summaries are based on a novel mechanism that combines text labels
and bisimulation contractions. The labels assigned to resources
in RDF graphs are hierarchical, enabling summarization at different granularities. The bisimulation contractions are applied
to subgraphs defined via queries, providing for summarization of
arbitrary large or small graph neighbourhoods. Our tool also
generates SPARQL queries from summaries.
",,
12/27/2020 20:49:33,2010,Data Linking: Capturing and Utilising Implicit Schema-level Relations,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper07.pdf,Andriy Nikolov,Open University, United Kingdom,Victoria Uren,University of Sheffield, United Kingdom,Enrico Motta,Open University, United Kingdom,,,,,,,,,,,,,"Schema-level heterogeneity represents an obstacle for automated discovery of coreference resolution links between individuals. Although there is a multitude of existing schema
matching solutions, the Linked Data environment differs
from the standard scenario assumed by these tools. In particular, large volumes of data are available, and repositories
are connected into a graph by instance-level mappings. In
this paper we describe how these features can be utilised to
produce schema-level mappings which facilitate the instance
coreference resolution process. Initial experiments applying
this approach to public datasets have produced encouraging
results.
",H.4.m [Information Systems]: Miscellaneous; D.2 [Software]: Software Engineering,"Data fusion, coreference resolution, linked data"
12/27/2020 21:06:03,2010,Preserving Linked Data on the Semantic Web by the application of Link Integrity techniques from Hypermedia,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper08.pdf,Rob Vesse,University of Southampton, United Kingdom,Wendy Hall,University of Southampton, United Kingdom,Leslie Carr,University of Southampton, United Kingdom,,,,,,,,,,,,,"As the Web of Linked Data expands it will become increasingly important to preserve data and links such that the
data remains useful. In this work we present a method for
locating linked data to preserve which functions even when
the URI the user wishes to preserve does not resolve (i.e.
is broken/not RDF) and an application for monitoring and
preserving the data. This work is based upon the principle
of adapting ideas from hypermedia link integrity in order to
apply them to the Semantic Web.","Semantic Web, Linked Data, Link Integrity, Preservation",
12/27/2020 21:13:05,2010,When owl:sameAs isn't the Same: An Analysis of Identity Links on the Semantic Web,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper09.pdf,Harry Halpin,University of Edinburgh, United Kingdom,Patrick J. Hayes,Institute for Human and Machine Cognition, US,,,,,,,,,,,,,,,,"In Linked Data, the use of owl:sameAs is ubiquitous in
‚Äòinter-linking‚Äô data-sets. However, there is a lurking suspicion within the Linked Data community that this use of
owl:sameAs may be somehow incorrect, in particular with
regards to its interactions with inference. In fact, owl:sameAs
can be considered just one type of ‚Äòidentity link,‚Äô a link that
declares two items to be identical in some fashion. After
reviewing the definitions and history of the problem of identity in philosophy and knowledge representation, we outline
four alternative readings of owl:sameAs, showing with examples how it is being (ab)used on the Web of data. Then
we present possible solutions to this problem by introducing
alternative identity links that rely on named graphs.
","Linked Data, ontology, resource, Web architecture",H.3.d [Information Technology and Systems]: Metadata
12/27/2020 21:15:51,2010,Linking Data from RESTful Services,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper10.pdf,Rosa Alarcon,Pontificia Universidad Catolica de Chile, Chile,Erik Wilde,School of Information, US,,,,,,,,,,,,,,,,"One of the main goals of the Semantic Web is to extend current human-readable Web resources with semantic information encoded in a machine-processable form. One of its most
successful approaches is the Web of Data which by following the principles of Linked Data have made available several
data sources compliant with the Semantic Web technologies,
such as, RDF triple stores, and SPARQL endpoints. On the
other hand, the set of the architectural principles that underlie the human-readable Web has been conceptualized as the
Representational State Transfer (REST) architectural style.
In this paper, we distill REST concepts in order to provide a mechanism for describing REST (i.e. human-readable
Web) resources and transform them into semantic resources.
The strategy allowed us to harvest already existing Web resources without requiring changes on the original sources, or
ad-hoc interfaces. The presented strategy aims to contribute
to the availability of more semantic datasets and become a
further step to lower the entry barrier to semantic resources
publishing.","REST, Web Data, Crawling","H.3.5 [Information Storage and Retrieval]: Online Information Services‚ÄîWeb-based services, Data sharing"
12/27/2020 21:17:51,2010,A Proposal for Publishing Data Streams as Linked Data (A Position Paper),http://events.linkeddata.org/ldow2010/papers/ldow2010_paper11.pdf,Davide F. Barbieri,Politecnico di Milano, Italy,Emanuele Della Valle,Politecnico di Milano, Italy,,,,,,,,,,,,,,,,"Streams are appearing more and more often on the Web
in sites that distribute and present information in real-time
streams. We anticipate a rapidly growing need of mashing
up this streaming information with more static one. While
best practices for linking static data on the Web were published and facilitate the mash up of static information published on the Web, streams were neglected. In this short
position paper, we propose an approach to publish Data
Streams as Linked Data.","Data Streams, Linked Data, Virtual RDF, Stream Reasoning",
12/27/2020 21:19:04,2010,Towards Dataset Dynamics: Change Frequency of Linked Open Data Sources,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper12.pdf,Jurgen Umbrich,National University of Ireland, Ireland,Michael Hausenblas,National University of Ireland, Ireland,Aidan Hogan,National University of Ireland, Ireland,Axel Polleres,National University of Ireland, Ireland,Stefan Decker,National University of Ireland, Ireland,,,,,,,"Datasets in the LOD cloud are far from being static in
their nature and how they are exposed. As resources are
added and new links are set, applications consuming the
data should be able to deal with these changes. In this paper we investigate how LOD datasets change and what sensible measures there are to accommodate dataset dynamics.
We compare our findings with traditional, document-centric
studies concerning the ‚Äúfreshness‚Äù of the document collections and propose metrics for LOD datasets.
",,
12/27/2020 21:30:36,2010,An HTTP-Based Versioning Mechanism for Linked Data,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper13.pdf,Herbert Van de Sompel,Los Alamos National Laboratory, US,Robert Sanderson,Los Alamos National Laboratory, US,Michael L. Nelson,Old Dominion University, US,Lyudmila L. Balakireva,Los Alamos National Laboratory, US,Harihar Shankar,Los Alamos National Laboratory, US,Scott Ainsworth,Old Dominion University, US,,,,"Dereferencing a URI returns a representation of the current
state of the resource identified by that URI. But, on the
Web representations of prior states of a resource are also
available, for example, as resource versions in Content Management Systems or archival resources in Web Archives such
as the Internet Archive. This paper introduces a resource
versioning mechanism that is fully based on HTTP and uses
datetime as a global version indicator. The approach allows
‚Äúfollow your nose‚Äù style navigation both from the current
time-generic resource to associated time-specific version resources as well as among version resources. The proposed
versioning mechanism is congruent with the Architecture of
the World Wide Web, and is based on the Memento framework that extends HTTP with transparent content negotiation in the datetime dimension. The paper shows how the
versioning approach applies to Linked Data, and by means
of a demonstrator built for DBpedia, it also illustrates how it
can be used to conduct a time-series analysis across versions
of Linked Data descriptions.","Web Architecture, HTTP, Linked Data, Resource Versioning, Web Archiving, Temporal Applications",H.3.5 [Information Storage and Retrieval]: Online Information Services
12/27/2020 21:32:48,2010,Linking UK Government Data,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper14.pdf,John Sheridan,The National Archives, United Kingdom,Jeni Tennison,The Stationery Office, United Kingdom,,,,,,,,,,,,,,,,"What does it take to create a web of linked government data?
With the launch of data.gov.uk the UK Government has been
finding out. This paper sets out the case for using Linked Data
standards for publishing open government data and describes
some of the benefits. It explains how Linked Data standards
uniquely allow governments to publish data responsibly and why
responsible data publishing is so important to the open
government data movement. The paper goes on to explain how
the Linked Data world was not quite ready for the large-scale
adoption of these standards by a major government, leaving much
to be done to develop practical approaches and patterns for the
publishing of government data. From URIs, to provenance and
versioning, through to statistics and geographic information,
much thinking and work has been done. In each case the emphasis
has been, not on research, but designing simple repeatable
patterns, supported through tools. This work has also involved
and building understanding and capability amongst officials from
across government departments and agencies.
It explains why the government's use of linked data standards was
not universally welcomed and was even greeted by antagonism
from some. Learning from this feedback the paper describes how
we are now using linked data standards to enable government as a
platform, commoditising the process of creating APIs to meet the
needs of a wide range of data consumers, from business, academia
and the developer communities.","Linked Data, eGovernment",H.4 [Information Systems]: Information Systems Applications
12/27/2020 21:36:36,2010,Geographical Service: a compass for the Web of Data.,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper15.pdf,Gianluca Correndo,University of Southampton, United Kingdom,Manuel Salvadores,University of Southampton, United Kingdom,Yang Yang,University of Southampton, United Kingdom,Nicholas Gibbins,University of Southampton, United Kingdom,Nigel Shadbolt,University of Southampton, United Kingdom,,,,,,,"This paper describes a Linked Data service that supports
the navigation and retrieval of geographical entities for the
UK territory. Geographical entities, in the extent of this
paper, are linked data resources that describe objects that
have a geographical extension. The service presented in this
paper allows the querying of resources that contain or are
contained by a given entity URI. The recent publication of
UK Public Sector Information (PSI) data sets has brought
to the attention of the community the redundant presence
of location based context. At the same time it stresses the
inadequacy of current Linked Data services for exploiting
the semantics of such contextual dimensions for easing entity retrieval and browsing. We present an approach for a
geography based service that helps in querying qualitative
spatial relations for the UK geography (proper containment
so far). We also provide an exploitation scenario based on
a backlinking service and PSI Open Linked Data, published
within the EnAKTing project.
","Linked Data, geographical reasoning, Web of Data",H.3.4 [Systems and Software]: Distributed systems; H.5.4 [Web]: Navigation; H.3.5 [Online Information Services]: Web-based services
12/27/2020 21:38:01,2010,Real-time #SemanticWeb in <= 140 chars,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper16.pdf,Joshua Shinavier,Rensselaer Polytechnic Institute, US,,,,,,,,,,,,,,,,,,,"Instant publication, along with the prospect of widespread
geotagging in microblog posts, presents new opportunities
for real-time and location-based services. Much as these services are changing the nature of search on the World Wide
Web, so the Semantic Web is certain to be both challenged
and enhanced by real-time content. This paper introduces a
semantic data aggregator which brings together a collection
of compact formats for structured microblog content with
Semantic Web vocabularies and best practices in order to
augment the Semantic Web with real-time, user-driven data.
Emerging formats, modeling issues, publication and data
ownership of microblogging content, and basic techniques
for real-time, real-place semantic search are discussed.",,
12/27/2020 21:40:46,2010,User Interface Design Considerations for Linked Data Authoring Environments,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper17.pdf,Stephen Davies,University of Mary Washington, US,Jesse Hatfield,University of Mary Washington, US,Chris Donaher,University of Mary Washington, US,Jessica Zeitz,University of Mary Washington, US,,,,,,,,,,"If non-technical end users are to contribute to the Web of Data
as they have to the Web of Documents, they must employ tools
that enable them to do so. This challenge is not easy to meet, as
formal knowledge representation is a daunting task for the
uninitiated. Indeed, we have empirically observed that
expressing anything but the most straightforward of facts in
RDF-compatible format is extremely difficult for newcomers to
do reliably.
This paper reports on a controlled experiment in which novices
attempted to use a prototype Linked Data interface to both find
and encode bits of everyday knowledge. The application
presents a user-friendly veneer to the Semantic Web,
manifesting the essential graph-based nature of the data model
while shielding the user from the complexity of syntax. This
allows us to study user behavior in attacking the deep, cognitive
problem: breaking down knowledge into the triple-based
structure required by RDF Linked Data. Our study sheds light on
some of the key aspects of knowledge formulation that novices
struggle with, and suggests several specific design approaches
for Linked Data authoring environments that our experiment
makes clear beneficially address crucial issues.",,H.5.2 [User Interfaces]: Interaction styles; H.5.4 [Hypertext/ Hypermedia]: User issues.
12/27/2020 21:43:10,2010,Looking for Experts? What can Linked Data do for You?,http://events.linkeddata.org/ldow2010/papers/ldow2010_paper19.pdf,Milan Stankovic,Université Paris, France,Claudia Wagner,JOANNEUM RESEARCH, Austria,Jelena Jovanovic,University of Belgrade, Serbia,Philippe Laublet,Université Paris, France,,,,,,,,,,"Expert search and profiling systems aim to identify candidate
experts and rank them with respect to their estimated expertise on
a given topic, using available evidence. Traditional expert search
and profiling systems exploit structured data from closed systems
(e.g. email program) or unstructured data from open systems (e.g.
the Web). However, on today‚Äôs Web, there is a growing number
of data sets published according to the Linked Data principals, the
majority of them being part of the Linked Open Data (LOD)
cloud. As LOD connects data and people across different
platforms in a meaningful way, one can assume that expert search
and profiling systems would benefit from harnessing LOD. The
work presented in this paper sets out to prove this assumption and
to explore potential benefits and drawbacks of using the LOD
cloud as expertise evidence source. We conducted several
experiments to evaluate the feasibility of existing expert search
and profiling approaches on a recent snapshot of the LOD cloud.
Our findings indicate that LOD cloud is already a useful source
for some kinds of expert search approaches (e.g., those based on
publications and professional events) but still has to meet certain
requirements in order to reach its full potential.",,
12/27/2020 21:45:44,2010,Discovery and Construction of Authors Profile from Linked Data (A case study for Open Digital Journal),http://events.linkeddata.org/ldow2010/papers/ldow2010_paper18.pdf,Atif Latif,Institute for Knowledge Management, Austria,Muhammad Tanvir Afzal,Institute for Information Systems and Computer Media, Austria,Denis Helic,Institute for Knowledge Management, Austria,Klaus Tochtermann,Institute for Knowledge Management, Austria,Hermann Maurer,Institute for Information Systems and Computer Media, Austria,,,,,,,"The Open access digital journals, motivated from open access
movement, now play a vital role in the dissemination of scientific
literature and author‚Äôs information over the web. In digital
journals‚Äô environment, a well-linked collection of electronic
resources is of great importance especially in creating
opportunities for collaborations between organization, institutions,
and persons. Finding information about authors (authors‚Äô profiles)
in a digital journal‚Äôs environment is crucial to increase the overall
productivity and unprecedented success. Inspired from Linked
Open Data (LOD) initiative, we have developed a tool which can
establish links between authors of digital journals with relevant
semantic resources available in LOD. The proposed system is able
to disambiguate authors and can: 1) locate, 2) retrieve, and 3)
structure the relevant semantic resources. Furthermore, the system
constructs comprehensive aspect oriented authors‚Äô profiles from
heterogeneous datasets of LOD on the fly. This paper investigates
the potentials of such an approach on a digital journal known as
Journal of Universal Computer Science (J.UCS). It is our strong
belief that this kind of applications can motivate researchers and
developers to investigate different application areas where Linked
Open Data can contribute, bring added value, and can take the
idea of open access further.","Linked Data, Semantic Web, Digital Journals, Linked Data Mining, Concept Aggregation, DBpedia. ","H.3.3 [Information Search and Retrieval]: Information filtering; I.2.8 [Problem Solving, Control Methods, and Search]: Heuristic methods. "
12/27/2020 21:47:11,2011,A Privacy Preference Ontology (PPO) for Linked Data,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper01-sacco.pdf,Owen Sacco,National University of Ireland, Ireland,Alexandre Passant,National University of Ireland, Ireland,,,,,,,,,,,,,,,,"Linked Data enables people to access other users‚Äô data stored
in several places, distributed across the Web. Current Linked
Data mechanisms mostly provide an open environment where
all data is freely accessible, which could discourage some
people to provide sensitive data in the Linking Open Data
(LOD) cloud. Although the existing Web Access Control
(WAC) vocabulary restricts RDF documents to specified
users, it does not provide fine-grained privacy measures which
specify complex restrictions to access the data. In this paper, we propose a lightweight vocabulary ‚Äî on top of WAC
‚Äî called the Privacy Preference Ontology (PPO) that enables users to create fine-grained privacy preferences for their
data. The vocabulary is designed to restrict any resource to
certain attributes which a requester must satisfy","Privacy, Linked Data, WebID, Web Access Control, FOAF, RDF, Named Graphs",1.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Method; K.4.1 [Public Policy Issues]: Privacy; K.6.5 [Management of Computing and Information Systems]: Security and Protection
12/27/2020 21:51:18,2011,Publishing Provenance Information on the Web using the Memento Datetime Content Negotiation,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper02-coppens.pdf,Sam Coppens,Ghent University ,Belgium,Erik Mannens,Ghent University ,Belgium,Davy Van Deursen,Ghent University ,Belgium,Patrick Hochstenbach,Boekentoren ,Belgium,Bart Janssens,Descartes Systems Group,Belgium,Rik Van de Walle,Ghent University ,Belgium,,,,"In Belgium, we developed a digital long-term preservation
archive to preserve the information from our heritage institutions. This platform harvests the information from the institutions, preserves the information for the long term and
disseminates the information as Linked Open Data. Our
platform produces many different versions of the harvested
data to keep the information accessible over time when, e.g.,
mapping the metadata or transcoding the multimedia files,
but it also produces a lot of provenance information relating
all those different versions of a resource. For publishing this
information as Linked Open Data, we extended our Linked
Open Data server with Memento datetime content negotiation. Next to this, we extended the Memento framework to
also publish the provenance information of those datetime
content negotiated versions using an HTTP provenance link
header for automatic discovery of the provenance information. This way, our framework allows to publish the information of a resource as Linked Open Data, including all
its previous versions and their provenance information, in a
web-accessible manner.","Linked Open Data, Memento datetime content negotiation, Provenance",H.4 [Information Systems Applications]: General
12/27/2020 21:52:57,2011,Augmenting the Web of Data using Referers,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper03-muehleisen.pdf,Hannes Mühleisen,Freie Universität Berlin, Germany,Anja Jentzsch,Freie Universität Berlin, Germany,,,,,,,,,,,,,,,,"Linked Data relies on one central concept: Typed links connect entities stored within data sets published by different
individuals. Manual input and mapping are common techniques to create these links. We propose a novel method,
where HTTP Referer information is used to create new links
between Linked Data entities stored in different data sets.
We evaluate our method using 27.86 million real-world log
entries from web servers hosting Linked Data.",,
12/27/2020 22:10:08,2011,RESTful writable APIs for the web of Linked Data using relational storage solutions,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper04-garrote.pdf,Antonio Garrote,Universidad de Salamanca, Spain,María N. Moreno García,Universidad de Salamanca, Spain,,,,,,,,,,,,,,,,"Linked Data is rapidly becoming an important mechanism to
expose structured data in the web. The ability to inter-link
data sets from different providers using standard description
vocabularies and the same data model, opens new possibilities in the way these data can be used. Despite of its growth,
Linked Data principles have not found yet widespread application in the design of data APIs for web applications. The
lack of a write support for linked data repositories, the barrier imposed by the required technological change and the
immature state of client and server semantic infrastructure
are some of the main causes for this lack of adoption.
This paper introduces a possible alternative for building
writable web APIs according to Linked Data principles, using the already deployed technology stack present in most
web applications. We propose the use of R2RML to lift relational data into the RDF model as well as to map SQL manipulation data queries into SPARQL update queries. Additionally a RDF vocabulary describing a RESTful interface
for the mapped data that can be easily consumed from web
clients is proposed. The combination of both aspects allows web developers to offer a familiar web API compatible
with linked data APIs that can be deployed along with the
already existing interface.",,H.3.5 [Online Information Services]: Web-based services; D.2.12 [Interoperability]: Data mapping
12/27/2020 22:11:34,2011,How Caching Improves Efficiency and Result Completeness for Querying Linked Data,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper05-hartig.pdf,Olaf Hartig,Universität zu Berlin , Germany,,,,,,,,,,,,,,,,,,,"Link traversal based query execution is a novel query approach
which enables applications that exploit the Web of Data to its full
potential. This approach makes use of the characteristics of Linked
Data: During query execution it traverses data links to discover
data that may contribute to query results. Once retrieved from the
Web, the data can be cached and reused for subsequent queries.
We expect such a reuse to be beneficial for two reasons: First, it
may improve query performance because it reduces the need to retrieve data multiple times; second, it may provide for additional
query results, calculated based on cached data that would not be
discoverable by a link traversal based execution alone. However,
no systematic analysis exist that justifies the application of caching
strategies based on these assumptions.
In this paper we evaluate the potential of caching to improve
efficiency and result completeness in link traversal based query execution systems. We conceptually analyze the potential benefit of
keeping and reusing retrieved data. Furthermore, we verify the theoretical impact of caching by conducting a comprehensive experiment that is based on a real-world application scenario. ",,
12/27/2020 22:19:56,2011,A Main Memory Index Structure to Query Linked Data,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper06-hartig.pdf,Olaf Hartig,Universität zu Berlin , Germany,Frank Huber,Universität zu Berlin, Germany,,,,,,,,,,,,,,,,"A possible approach to query Linked Data combines the actual
evaluation of a query with the traversal of data links in order to
discover and retrieve potentially relevant data. An implementation
of this idea requires approaches that support an efficient and flexible management of temporary, ad hoc data collections that emerge
during query execution. However, existing proposals for managing
RDF data primarily focus on persistent storage and query execution
for large datasets and, thus, are unsuitable in our dynamic scenario
which involves many small sets of data.
In this paper we investigate main memory data structures to store
Linked Data in a query execution system. We discuss the requirements for such a data structure, introduce three strategies that make
use of hash table based indexes, and compare these strategies empirically. While this paper focuses on our query execution approach, the discussed data structures can also be applied to other
approaches that retrieve Linked Data from remote sources via URI
look-ups in order to process this data locally. ",,
12/27/2020 22:33:59,2011,LiDDM: A Data Mining System for Linked Data,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper07-pavan.pdf,Venkata Narasimha,Indian Institute of Information, India,Pavan Kappara,Indian Institute of Information, India,Ryutaro Ichise,National Institute of Informatics, Japan,O.P. Vyas,Indian Institute of Information, India,,,,,,,,,,"In today‚Äôs scenario, the quantity of linked data is growing
rapidly. The data includes ontologies, governmental data,
statistics and so on. With more and more sources publishing the data, the amount of linked data is becoming enormous. The task of obtaining the data from various sources,
integrating and fine-tuning the data for desired statistical
analysis assumes prominence. So there is need of a good
model with efficient UI design to perform the Linked Data
Mining. We proposed a model that helps to effectively interact with linked data present in the web in structured format,
retrieve and integrate data from different sources, shape and
fine-tune the so formed data for statistical analysis, perform
data mining and also visualize the results at the end.
",,
12/27/2020 22:38:54,2011,Talash : Friend Finding In Federated Social Networks,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper08-dhekane.pdf,Ruturaj Dhekane,Indian Institute Of Technology, India,Brion Vibber,StatusNet, Canada,,,,,,,,,,,,,,,,"In large online social networks, Friend Recommendation has
evolved into an interesting problem. We try to find known
acquaintances and new interesting friends on a Federated
Social Network (FSN) , using StatusNet as our platform.
FSNs are decentralized networks on the internet which can
interoperate using the OStatus Suite of protocols. Friend
finding on these networks is hard because we do not know
the existence of other social networks or Users. We show
how Linked Data representation like FOAF can solve this
problem.
We devise a model for the Federated Network centered
around a User and use it to define the problem of Friend
Finding. The solution uses two phases, first known as Quick
Connect which tries to find old acquaintances. The second
phase, Delayed Connect uses the Social Graph of Users to
find prospective friends. We show how the FSN information
centered around a User can be extracted from FOAF entries
and generate new recommendations. We shall illustrate the
working of Talash as a part of StatusNet. We experimented
on the existing FSN and collected feedback from its Users.
The results are encouraging and open new avenues for Friend
Finding on the Internet.
To the best of our knowledge, this is the first study of Federated Social Networks and the problem of Friend Finding
in them.","Friend Recommendation, Federation, Social Networks",E.1 [Data Structures]: Graphs and networks; H.3.5 [Online Information Services]: Web-based services
12/27/2020 22:53:25,2011,Automatically Annotating Text with Linked Open Data,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper09-rusu.pdf,Delia Rusu,Jo_ef Stefan Institute, Slovenia,Bla_ Fortuna,Jo_ef Stefan Institute, Slovenia,Dunja Mladeni_,Jo_ef Stefan Institute, Slovenia,,,,,,,,,,,,,"This paper presents and evaluates two existing word sense
disambiguation approaches which are adapted to annotate text
with several popular Linked Open Data datasets. One of the
algorithms is based on relationships between resources, while the
other one takes advantage of resource definitions provided by the
datasets. The aim is to test their applicability when annotating text
with resources from WordNet, OpenCyc and DBpedia. The
experiments expose several shortcomings related to the current
approaches, which are mostly connected to overfitting the
datasets. Based on the findings, we indicate future work directions
regarding text annotation with Linked Open Data resources,
which can bridge these shortcomings","Linked Open Data, text annotation, word sense disambiguation.",I.2.7 [Natural Language Processing]: Text analysis
12/27/2020 23:03:25,2011,Identifying Relevant Sources for Data Linking using a Semantic Web Index,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper10-nikolov.pdf,Andriy Nikolov,Open University, United Kingdom,Mathieu d’Aquin,Open University, United Kingdom,,,,,,,,,,,,,,,,"With more data repositories constantly being published on
the Web, choosing appropriate data sources to interlink with
newly published datasets becomes a non-trivial problem.
While catalogs of data repositories and meta-level descriptors such as VoiD provide valuable information to take these
decisions, more detailed information about the instances included into repositories is often required to assess the relevance of datasets and the part of the dataset to link to.
However, retrieving and processing such information for a
potentially large number of datasets is practically unfeasible.
In this paper, we examine how using an existing semantic
web index can help identifying candidate datasets for linking. We further apply ontology schema matching techniques
to rank these candidate datasets and extract the sub-dataset
to use for linking, in the form of classes with instances more
likely to match the ones of the local dataset.","Data fusion, data linking, linked data",H.3.3 [Information Systems]: Information Storage and RetrievalInformation Search and Retrieval
12/27/2020 23:13:31,2011,Re-using Cool URIs: Entity Reconciliation Against LOD Hubs,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper11-maali.pdf,Fadi Maali,Digital Enterprise Research Institute, Ireland,Richard Cyganiak,Digital Enterprise Research Institute, Ireland,Vassilios Peristeras,European Commission, Europe,,,,,,,,,,,,,"We observe that ‚ÄúLOD hubs‚Äù are emerging. They provide
well-managed reference identifiers that attract a large share
of the incoming links on the Web of Data and play a crucial
role in data integration within communities of interest. But
connecting to such hubs as part of the Linked Data publishing process is still a difficult task. In this paper, we explore
several approaches to the implementation of reconciliation
services that allow third-party publishers to link their data
to LOD hubs as part of the data publishing process. We
evaluate four approaches using the OAEI Instance Matching Benchmark, and describe their implementation in an extension to the popular data workbench application Google
Refine.
","Linked Data, Google Refine, reconciliation, LOD, URI, entity matching, SPARQL, link generation, Silk",
12/27/2020 23:15:22,2011,Open eBusiness Ontology Usage: Investigating Community Implementation of GoodRelations,http://events.linkeddata.org/ldow2011/papers/ldow2011-paper12-ashraf.pdf,Jamshaid Ashraf,Curtin University of Technology, Australia,Richard Cyganiak,National University of Ireland, Ireland,Sean O’Riain,National University of Ireland, Ireland,Maja Hadzic,Curtin University of Technology, Australia,,,,,,,,,,"The GoodRelations Ontology is experiencing the first stages of
mainstream adoption, with its appeal to a range of enterprises as
the eCommerce ontology of choice to promote its product
catalogue. As adoption increases, so too does the need to review
and analyze current implementation of the ontology to better
inform future usage and uptake. To comprehensively understand
the implementation approaches, usage patterns, instance data and
model coverage, data was collected from 105 different web based
sources that have published their business and product-related
information using the GoodRelations Ontology. This paper
analyses the ontology usage in terms of data instantiation, and
conceptual coverage using SPARQL queries to evaluate quality,
usefulness and inference provisioning. Experimental results
highlight that early publishers of structured eCommerce data
benefit more due to structured data being more readily search
engine indexable, but the lack of available product ontologies and
product master datasheets is impeding the creation of a
semantically interlinked eCommerce Web.","GoodRelations, Linked Data, Instance data analysis, Business ontology, Structured eCommerce data, Ontology usage.",D.2.5 [Software/Software Engineering]: Testing and Debugging
12/27/2020 23:26:21,2012,Synote: Weaving Media Fragments and Linked Data,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-01.pdf,Yunjia Li,University of Southampton, United Kingdom,Mike Wald,University of Southampton, United Kingdom,Tope Omitola,University of Southampton, United Kingdom,Nigel Shadbolt,University of Southampton, United Kingdom,Gary Wills,University of Southampton, United Kingdom,,,,,,,"While end users could easily share and tag the multimedia resources online, the searching and reusing of the inside content of multimedia, such as a certain area within an
image or a ten minutes segment within a one-hour video,
is still difficult. Linked data is a promising way to interlink media fragments with other resources. Many applications in Web 2.0 have generated large amount of external
annotations linked to media fragments. In this paper, we
use Synote as the target application to discuss how media
fragments could be published together with external annotations following linked data principles. Our design solves
the dereferencing, describing and interlinking methods problems in interlinking multimedia. We also implement a model
to let Google index media fragments which improves media
fragments‚Äô online presence. The evaluation shows that our
design can successfully publish media fragments and annotations for both semantic Web agents and traditional search
engines. Publishing media fragments using the design we
describe in this paper will lead to better indexing of multimedia resources and their consequent findability.","annotation,linked data,media fragment,schema.org,search","I.7.2 [Document Preparation]: Hypertext/hypermedia, Multi/mixed media, Standards; H.5.m [Information Systems]: Miscellaneous"
12/27/2020 23:28:33,2012,NERD meets NIF: Lifting NLP Extraction Results to the Linked Data Cloud,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-02.pdf,Giuseppe Rizzo,EURECOM, France,Raphaël Troncy,EURECOM, France,Sebastian Hellmann,Universität Leipzig, Germany,,,,,,,,,,,,,"We have often heard that data is the new oil. In particular,
extracting information from semi-structured textual documents on the Web is key to realize the Linked Data vision.
Several attempts have been proposed to extract knowledge
from textual documents, extracting named entities, classifying them according to pre-defined taxonomies and disambiguating them through URIs identifying real world entities.
As a step towards interconnecting the Web of documents via
those entities, different extractors have been proposed. Although they share the same main purpose (extracting named
entity), they differ from numerous aspects such as their underlying dictionary or ability to disambiguate entities. We
have developed NERD, an API and a front-end user interface powered by an ontology to unify various named entity
extractors. The unified result output is serialized in RDF
according to the NIF specification and published back on
the Linked Data cloud. We evaluated NERD with a dataset
composed of five TED talk transcripts, a dataset composed
of 1000 New York Times articles and a dataset composed of
the 217 abstracts of the papers published at WWW 2011.","Named Entity extractors, Information extraction, Linked Data, Evaluation",I.2.7 [Artificial Intelligence]: [Natural Language Processing - Language parsing and understanding]
12/27/2020 23:29:56,2012,Towards Interoperable Provenance Publication on the Linked Data Web,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-03.pdf,Jun Zhao,University of Oxford, United Kingdom,Olaf Hartig,Universität zu Berlin, Germany,,,,,,,,,,,,,,,,"Provenance provides vital information for evaluating quality
and trustworthiness of information on the Web. To achieve
this we must have access to semantically interchangeable
provenance information and an agreement on where and how
this information is to be located. The ongoing W3C Provenance Working Group provides a promise towards leveraging these problems. In this position paper, we provide an
overview of how the upcoming standards and the existing
vocabularies and publication approaches could fit together
so that we achieve an optimal interoperability now and in
the near future. Because the standardization is an ongoing effort, any analysis results presented in this paper are
positional and are aimed at communicating the latest development of the working group to the community","Provenance, Linked Data, Semantic Web, RDF",H.4 [Information Systems Applications]: Genera
12/27/2020 23:32:06,2012,Using read/write Linked Data for Application Integration Towards a Linked Data Basic Profile,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-04.pdf,Arnaud J Le Hors,IBM, US,Martin Nally,IBM, US,Steve K Speicher,IBM, US,,,,,,,,,,,,,"Linked Data, as defined by Tim Berners-Lee‚Äôs 4 rules [1], has
enjoyed considerable well-publicized success as a technology for
publishing data in the World Wide Web [2]. The Rational group in
IBM has for several years been employing a read/write usage of
Linked Data as an architectural style for integrating a suite of
applications, and we have shipped commercial products using this
technology. We have found that this read/write usage of Linked
Data has helped us solve several perennial problems that we had
been unable to successfully solve with other application
integration architectural styles that we have explored in the past.
The applications we have integrated in IBM are primarily in the
domains of Application Lifecycle Management (ALM) and
Integration System Management (ISM), but we believe that our
experiences using read/write Linked Data to solve application
integration problems could be broadly relevant and applicable
within the IT industry.
This paper explains why Linked Data, which builds on the
existing World Wide Web infrastructure, presents some unique
characteristics, such as being distributed and scalable, that may
allow the industry to succeed where other application integration
approaches have failed. It discusses lessons we have learned along
the way and some of the challenges we have been facing in using
Linked Data to integrate enterprise applications.
Finally, we discuss several areas that could benefit from
additional standard work and discuss several commonly
applicable usage patterns along with proposals on how to address
them using the existing W3C standards in the form of a Linked
Data Basic Profile. This includes techniques applicable to clients
and servers that read and write linked data, a type of container
that allows new resources to be created using HTTP POST and
existing resources to be found using HTTP GET (analogous to
things like Atom Publishing Protocol (APP) [3]).
","Linked Data, Usage Patterns, Application Integration, Enterprise Application, Standards, ALM, ISM, Profile",
12/27/2020 23:33:41,2012,Linked Data Access Goes Mobile: Context-Aware Authorization for Graph Stores,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-05.pdf,Luca Costabello,INRIA, France,Serena Villata,INRIA, France,Nicolas Delaforge,INRIA, France,Fabien Gandon,INRIA, France,,,,,,,,,,"To encourage data providers to publish a maximum of data
on the Web, we propose a mechanism to define lightweight
access control policies for graph stores. Influenced by the
steep growth of the mobile web, our Linked Data access
control framework features context-aware control policies.
The proposed framework is exclusively grounded on standard
Semantic Web languages. The framework architecture is
designed as a pluggable filter for generic SPARQL endpoints,
and it has been evaluated on a test dataset.
","Linked Data, Ubiquitous Web, Access Control",I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods; K.6.5 [Management of Computing and Information Systems]: Security and Protection
12/28/2020 14:25:18,2012,Querying the Web of Interlinked Datasets using VOID Descriptions,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-06.pdf,Ziya Akar,Ege University, Turkey,Tayfun Gökmen Halaç,Ege University, Turkey,Erdem Eser Ekinci,Ege University, Turkey,Oguz Dikenelli,Ege University, Turkey,,,,,,,,,,"Query processing is an important way of accessing data on
the Semantic Web. Today, the Semantic Web is characterized as a web of interlinked datasets, and thus querying the
web can be seen as dataset integration on the web. Also, this
dataset integration must be transparent from the data consumer as if she is querying the whole web. To decide which
datasets should be selected and integrated for a query, one
requires a metadata of the web of data. In this paper, to
enable this transparency, we introduce a federated query engine called WoDQA (Web of Data Query Analyzer) which
discovers datasets relevant with a query in an automated
manner using VOID documents as metadata. WoDQA focuses on powerful dataset elimination by analyzing query
structure with respect to the metadata of datasets. Dataset
and linkset descriptions in VOID documents are analyzed for
a SPARQL query and a federated query is constructed. By
means of linkset concept of VOID, links between datasets are
incorporated into selection of federated data sources. Current version of WoDQA is available as a SPARQL endpoint.",,
12/28/2020 14:27:02,2012,SPARQL Query Mediation over RDF Data Sources with Disparate Contexts,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-07.pdf,Xiaoqing Zheng,Fudan University, China,Stuart E. Madnick,Massachusetts Institute of Technology, US,Xitong Li,Massachusetts Institute of Technology, US,,,,,,,,,,,,,"Many Semantic Web applications require the integration of data
from distributed and autonomous RDF data sources. However, the
values in the RDF triples would be frequently recorded simply as
the literal, and additional contextual information such as unit and
format is often omitted, relying on consistent understanding of the
context. In the wider context of the Web, it is generally not safe to
make this assumption. The Context Interchange strategy provides
a systematic approach for mediated data access in which semantic
conflicts among heterogeneous data sources are automatically
detected and reconciled by a context mediator. In this paper, we
show that SPARQL queries that involve multiple RDF graphs
originating from different contexts can be mediated in the way
using the Context Interchange (COIN) Framework. ","Semantic heterogeneity, semantic interoperability, query mediator, data integration ",H.2.5 [Database Management]: Heterogeneous Databases ‚Äì data translation; H.2.4 [Database Management]: Systems ‚Äì query processing; H.3.5 [Information Storage and Retrieval]: Online Information Services ‚Äì data sharing
12/28/2020 14:28:05,2012,Holistic and Scalable Ontology Alignment for Linked Open Data,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-08.pdf,Toni Gruetze,Hasso Plattner Institute, Germany,Christoph Böhm,Hasso Plattner Institute, Germany,Felix Naumann,Hasso Plattner Institute, Germany,,,,,,,,,,,,,"The Linked Open Data community continuously releases
massive amounts of RDF data that shall be used to easily create applications that incorporate data from different
sources. Inter-operability across different sources requires
links at instance- and at schema-level, thus connecting entities on the one hand and relating concepts on the other
hand. State-of-the-art entity- and ontology-alignment methods produce high quality alignments for two ‚Äúnicely structured‚Äù individual sources, where an identification of relevant
and meaningful pairs of ontologies is a precondition. Thus,
these methods cannot deal with heterogeneous data from
many sources simultaneously, e.g., data from a linked open
data web crawl.
To this end we propose Holistic Concept Matching
(HCM). HCM aligns thousands of concepts from hundreds
of ontologies (from many sources) simultaneously, while
maintaining scalability and leveraging the global view on
the entire data cloud. We evaluated our approach against
the OAEI ontology alignment benchmark as well as on the
2011 Billion Triple Challenge data and present high precision results created in a scalable manner.",,
12/28/2020 14:29:55,2012,Benchmarking the Performance of Linked Data Translation Systems,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-09.pdf,Carlos R. Rivero,University of Sevilla, Spain,Andreas Schultz,Freie Universität Berlin, Germany,Christian Bizer,Freie Universität Berlin , Germany,David Ruiz,University of Sevilla, Spain,,,,,,,,,,"Linked Data sources on the Web use a wide range of different vocabularies to represent data describing the same type
of entity. For some types of entities, like people or bibliographic record, common vocabularies have emerged that are
used by multiple data sources. But even for representing
data of these common types, different user communities use
different competing common vocabularies. Linked Data applications that want to understand as much data from the
Web as possible, thus need to overcome vocabulary heterogeneity and translate the original data into a single target
vocabulary. To support application developers with this integration task, several Linked Data translation systems have
been developed. These systems provide languages to express
declarative mappings that are used to translate heterogeneous Web data into a single target vocabulary. In this paper, we present a benchmark for comparing the expressivity
as well as the runtime performance of data translation systems. Based on a set of examples from the LOD Cloud, we
developed a catalog of fifteen data translation patterns and
survey how often these patterns occur in the example set.
Based on these statistics, we designed the LODIB (Linked
Open Data Integration Benchmark) that aims to reflect the
real-world heterogeneities that exist on the Web of Data.
We apply the benchmark to test the performance of two
data translation systems, Mosto and LDIF, and compare
the performance of the systems with the SPARQL 1.1 CONSTRUCT query performance of the Jena TDB RDF store.",,D.2.12 [Interoperability]: Data mapping; H.2.5 [Heterogeneous Databases]: Data translation
12/28/2020 14:32:16,2012,UrbanMatch linking and improving Smart Cities Data,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-10.pdf,Irene Celino,CEFRIEL, Italy,Simone Contessa,CEFRIEL, Italy,Marta Corubolo,Politecnico di Milano, Italy,Daniele Dell’Aglio,CEFRIEL, Italy,Emanuele Della Valle,Politecnico di Milano, Italy,Stefano Fumeo,CEFRIEL, Italy,Thorsten Krüger,SIEMENS,Germany,"Urban-related data and geographic information are becoming mainstream in the Linked Data community due also to
the popularity of Location-based Services. In this paper, we
introduce the UrbanMatch game, a mobile gaming application that joins data linkage and data quality/trustworthiness
assessment in an urban environment. By putting together
Linked Data and Human Computation, we create a new interaction paradigm to consume and produce location-specific
linked data by involving and engaging the final user. The
UrbanMatch game is also offered as an example of value
proposition and business model of a new family of linked
data applications based on gaming in Smart Cities.",,
12/28/2020 14:34:03,2012,Automated interlinking of speech radio archives,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-11.pdf,Yves Raimond,BBC R&D, United Kingdom,Chris Lowis,BBC R&D, United Kingdom,,,,,,,,,,,,,,,,"The BBC is currently tagging programmes manually, using
DBpedia as a source of tag identifiers, and a list of suggested tags extracted from their synopsis. These tags are
then used to help navigation and topic-based search of BBC
programmes. However, given the very large number of programmes available in the archive, most of them having very
little metadata attached to them, we need a way of automatically assigning tags to programmes. We describe a framework to do so, using speech recognition, text processing and
concept tagging techniques. We evaluate this framework
against manually applied tags, and compare it with related
work. We find that this framework is good enough to bootstrap the interlinking process of archived content.
","Linked Data,Concept Tagging,Speech Processing",H.4 [Information Systems Applications]: Miscellaneous
12/28/2020 14:35:42,2012,Interacting with the Web of Data through a Web of Inter-connected Lenses,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-12.pdf,Igor Popov,University of Southampton, United Kingdom,m.c. schraefel,University of Southampton, United Kingdom,Gianluca Correndo,University of Southampton, United Kingdom,Wendy Hall,University of Southampton, United Kingdom,Nigel Shadbolt,University of Southampton, United Kingdom,,,,,,,"As a medium of structured information available on the
Web, Linked Data is still hard to access for most end users.
Current solutions facilitating end user access to Linked Data
are either thought the use of data-mapping approaches, which
allow configureable interfaces to be quickly deployed over
pre-selected aggregations of Linked Data, or enable users
themselves to browse the Web of Data through the use of
generic data browsers. While the first approach is useful and
promotes surfacing and easy repurposing of structured data
it does little to promote the use of linkages to other, remote
datasets. The second approach is much less useable for end
users, however enables them to experience browsing a interconnected Web of Data. In this paper we present mashpoint, a framework that aims to provide a middle ground
between both approaches. The approach treats data-centric
applications as high-level lenses over the data, and allows
selections of data to be pivoted between applications thus
facilitating navigation. The paper presents an initial prototype and discusses both implications and challenges in terms
of interaction and technology","End-user Interaction, Linked Data, User Interface",H5.2 [Information Interfaces and Presentation]: User Interfaces‚ÄîGraphical user interfaces (GUI); H5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia‚Äî User issues
12/28/2020 14:37:22,2012,Towards a Dynamic Linked Data Observatory,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-14.pdf,Tobias Käfer,Karlsruhe Institute of Technology, Germany,Jürgen Umbrich,National University of Ireland, Ireland,Aidan Hogan,National University of Ireland, Ireland,Axel Polleres,SIEMENS, Austria,,,,,,,,,,"We describe work-in-progress on the design and methodology of the Dynamic Linked Data Observatory: a framework
to monitor Linked Data over an extended period of time.
The core goal of our work is to collect frequent, continuous
snapshots of a subset of the Web of Data that is interesting
for further study and experimentation, with an aim to capture raw data about the dynamics of Linked Data. The resulting corpora will be made openly and continuously available to the Linked Data research community. Herein, we (1)
motivate the importance of such a corpus; (2) outline some of
the use-cases and requirements for the resulting snapshots;
(3) discuss different ‚Äúviews‚Äù of the Web of Data that affect
how we define a sample to monitor; (4) detail how we select
the scope of the monitoring experiment through sampling,
(5) discuss the final design of the monitoring framework that
will gather regular snapshots of (subsets of) the Web of Data
over the coming months and y",,
12/28/2020 18:29:29,2012,A Spectrometry of Linked Data,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-15.pdf,Giovanni Bartolomeo,University of Rome Tor Vergata , Italy,Stefano Salsano,University of Rome Tor Vergata, Italy,,,,,,,,,,,,,,,,"Entity mining is still a troublesome open problem. In past years
many approaches allowed to automate the generation of
equivalence links between references using schema matching or
various heuristics based on the recognition of similar property
values. In contrast, few of them considered the analysis of the
network of equivalence links (‚Äúequivalence network‚Äù) as an
indication of the likelihood and strength of the equivalence.
Following this basic idea, in this paper we apply the well known
Girvan and Newman algorithm to partition existing equivalence
networks into clusters of co-references and gain an insight of their
nature, size and composition. 
","Entity, Co-references, Linked Data",H [Information Systems]: Models and Principles; H.1 [Models and Principles]: Miscellaneous; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ‚Äî information filtering.
12/28/2020 18:32:07,2012,OWL: Yet to arrive on the Web of Data?,http://events.linkeddata.org/ldow2012/papers/ldow2012-paper-16.pdf,Birte Glimm,"Ulm University, Institute of Artificial Intelligence", Germany,Aidan Hogan,National University of Ireland, Ireland,Markus Krötzsch,University of Oxford, United Kingdom,Axel Polleres,SIEMENS, Austria,,,,,,,,,,"Seven years on from OWL becoming a W3C recommendation, and
two years on from the more recent OWL 2 W3C recommendation,
OWL has still experienced only patchy uptake on the Web. Although certain OWL features (like owl:sameAs) are very popular,
other features of OWL are largely neglected by publishers in the
Linked Data world. This may suggest that despite the promise of
easy implementations and the proposal of tractable profiles suggested in OWL‚Äôs second version, there is still no ‚Äúright‚Äù standard
fragment for the Linked Data community. In this paper, we (1)
analyse uptake of OWL on the Web of Data, (2) gain insights into
the OWL fragment that is actually used/usable on the Web, where
we arrive at the conclusion that this fragment is likely to be a simplified profile based on OWL RL, (3) propose and discuss such a
new fragment, which we call OWL LD (for Linked Data).
",,
12/28/2020 18:33:03,2012,Metadata Statistics for a Large Web Corpus,http://events.linkeddata.org/ldow2012/papers/ldow2012-inv-paper-1.pdf,Peter Mika,Yahoo! Research, Spain,Tim Potter,Yahoo! Research, Spain,,,,,,,,,,,,,,,,"We provide an analysis of the adoption of metadata standards on the Web based a large crawl of the Web. In particular, we look at what forms of syntax and vocabularies
publishers are using to mark up data inside HTML pages.
We also describe the process that we have followed and the
difficulties involved in web data extraction.
",,
12/28/2020 18:38:52,2012,Web Data Commons Extracting Structured Data from Two Large Web Corpora,http://events.linkeddata.org/ldow2012/papers/ldow2012-inv-paper-2.pdf,Hannes Mühleisen,Freie Universität Berlin, Germany,Christian Bizer,Freie Universität Berlin, Germany,,,,,,,,,,,,,,,,"More and more websites embed structured data describing for instance products, people, organizations, places, events, resumes, and
cooking recipes into their HTML pages using encoding standards
such as Microformats, Microdatas and RDFa. The Web Data Commons project extracts all Microformat, Microdata and RDFa data
from the Common Crawl web corpus, the largest and most up-todata web corpus that is currently available to the public, and provides
the extracted data for download in the form of RDF-quads. In this
paper, we give an overview of the project and present statistics about
the popularity of the different encoding standards as well as the
kinds of data that are published using each format.
",,
12/28/2020 18:41:20,2013,R&Wbase: Git for triples,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-01.pdf,Miel Vander Sande,Ghent University,Belgium,Pieter Colpaert,Ghent University,Belgium,Ruben Verborgh,Ghent University,Belgium,Sam Coppens,Ghent University,Belgium,Erik Mannens,Belgium, Germany,Rik Van de Walle,Ghent University,Belgium,,,,"Read/Write infrastructures are often predicted to be the next big
challenge for Linked Data. In the domains of Open Data and cultural
heritage, this is already an urgent need. They require the exchange
of partial graphs, personalised views on data and a need for trust.
A strong versioning model supported by provenance is therefore
crucial. However, current triple stores handle storage rather na√Øvely
and don not seem up for the challenge.
In this paper, we introduce R&Wbase, a new approach build on
the principles of distributed version control. Triples are stored in
a quad-store as consecutive deltas, reducing the amount of stored
triples drastically. We demonstrate an efficient technique for storing
different deltas in a single graph, allowing simple resolving of
different versions and separate access. Furthermore, provenance
tracking is included at operation level, since each commit, storing
a delta and its metadata, is described directly as provenance. The
use of branching is supported, providing flexible custom views on
the data. Finally, we provide a straightforward way for querying
different versions through SPARQL, by using virtual graphs.",,
12/28/2020 18:52:01,2013,OSLC Resource Shape: A language for defining constraints on Linked Data,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-02.pdf,Arthur G. Ryman,IBM, US,Arnaud J Le Hors,IBM, US,Steve Speicher,IBM, US,,,,,,,,,,,,,"IBM has for several years been employing a read/write usage of
Linked Data as an architectural style for integrating a suite of
applications. [1]
We are encouraged by the work done by the W3C Linked Data
Platform Working Group which is chartered to produce a W3C
Recommendation for HTTP-based (RESTful) application
integration patterns using read/write Linked Data .
The Linked Data Platform Recommendation will provide the
industry with a solid foundation to build on. Yet, more work will
need to be done to address in a standard way the needs of
enterprise solutions that use Linked Data as an application
integration platform. One such need is a type definition language
that can be used to communicate and validate constraints on RDF
data.
This paper explains the need for such a language, why standards
like RDFS and OWL are not suitable answers and, finally,
introduces OSLC Resource Shapes as a proposed solution.","Linked Data, Type Definition, Integrity Constraints, Validation, Application Integration, Standards",
12/28/2020 18:55:53,2013,Hydra: A Vocabulary for Hypermedia-Driven Web APIs,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-03.pdf,Markus Lanthaler,Graz University of Technology, Austria,Christian Gütl,Curtin University of Technology, Australia,,,,,,,,,,,,,,,,"Coping with the ever-increasing amount of data becomes
increasingly challenging. To alleviate the information overload
put on people, systems are progressively being connected directly
to each other. They exchange, analyze, and manipulate
humongous amounts of data without any human interaction. Most
current solutions, however, do not exploit the whole potential of
the architecture of the World Wide Web and completely ignore
the possibilities offered by Semantic Web technologies. Based on
the experiences gained by implementing and analyzing various
RESTful APIs and drawing from the longer history of Semantic
Web research we developed Hydra, a small vocabulary to describe
Web APIs. It aims to simplify the development of truly RESTful
services by leveraging the power of Linked Data. By breaking the
descriptions down into small independent fragments, a new breed
of interoperable Web APIs using decentralized, reusable, and
composable contracts can be realized.
",Web; Web service; Web API; HTTP; REST; Linked Data; RDF; vocabulary; ontology; Hydra,"H.3.4 [Information Storage and Retrieval]: Systems and Software ; Semantic Web, World Wide Web (WWW); H.4.3 [Information Systems Applications]: Communications Applications Internet; D.2.11 [Software]: Software Architectures Service-oriented architecture (SOA)"
12/28/2020 18:59:17,2013,Reasoning over SPARQL,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-08.pdf,Sam Coppens,Ghent University,Belgium,Miel Vander Sande,Ghent University,Belgium,Ruben Verborgh,Ghent University,Belgium,Erik Mannens,Ghent University,Belgium,Rik Van de Walle,Ghent University,Belgium,,,,,,,"Until now, the sparql query language was restricted to simple entailment. Now sparql is being extended with more
expressive entailment regimes. This allows to query over inferred, implicit knowledge. However, in this case the sparql
endpoint provider decides which inference rules are used for
its entailment regimes. In this paper, we propose an extension to the sparql query language to support remote
reasoning, in which the data consumer can define the inference rules. It will supplement the supported entailment
regimes of the sparql endpoint provider with an additional
reasoning step using the inference rules defined by the data
consumer. At the same time, this solution offers possibilities to solve interoperability issues when querying remote
sparql endpoints, which can support federated querying
frameworks. These frameworks can then be extended to
provide distributed, remote reasoning.
",,
12/28/2020 19:01:35,2013,Similar Structures inside RDF-Graphs,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-05.pdf,Anas Alzogbi,University of Freiburg, Germany,Georg Lausen,University of Freiburg, Germany,,,,,,,,,,,,,,,,"RDF is the common data model to publish structured data
on the Web. RDF data sets are given as subject-predicateobject triples and typically are represented as directed edgelabeled graphs. To make the information represented by
such graphs comprehensible, RDF-schema (RDFS) provides
concepts to define a class-structure as part of the given RDFgraph and thus supports a more abstract view on the data
set. In this paper we follow a different approach and propose
to make an RDF graph more comprehensible by reducing its
size by partitioning to discover subgraphs which are similar
with respect to their structure. The methods applied to derive a partition are based on bisimulation and agglomerative
clustering. We demonstrate the usefulness of the approach
by applying it on several synthetic and one real world RDF
datasets.",,
12/28/2020 19:03:21,2013,LHD: Optimising Linked Data Query Processing Using Parallelisation,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-06.pdf,Xin Wang,University of Southampton, United Kingdom,Thanassis Tiropanis,University of Southampton, United Kingdom,Hugh C. Davis,University of Southampton, United Kingdom,,,,,,,,,,,,,"In the past few years as large volume of Linked Data has
been published, and processing distributed SPARQL queries
over the Linked Data cloud is becoming increasingly challenging. The high data traffic cost and response time significantly affect the performance of distributed SPARQL
queries as the number of SPARQL end point and the volume
of data at each endpoint increase. In this context, parallelisation is promising to fully exploit the potential of connections to SPARQL endpoints and thus improve the efficiency
of querying Linked Data. We propose LHD, a distributed
SPARQL engine that is built on a highly parallel infrastructure and able to minimise query response time, and we
evaluate its performance using a BSBM based environment.","SPARQL, Linked Data, distributed query processing",C.2.4 [Computer-Communication Networks]: Distributed Systems‚ÄîDistributed databases; H.3.3 [Information Storage And Retrieval]: Systems and Software-Performance Evaluation
12/28/2020 19:16:04,2013,A Hybrid Approach to Linked Data Query Processing with Time Constraints,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-07.pdf,Steven Lynden,National Institute of Advanced Industrial Science and Technology, Japan,Isao Kojima,National Institute of Advanced Industrial Science and Technology, Japan,Akiyoshi Matono,National Institute of Advanced Industrial Science and Technology, Japan,Akihito Nakamura,National Institute of Advanced Industrial Science and Technology, Japan,Makoto Yui,National Institute of Advanced Industrial Science and Technology, Japan,,,,,,,"In addition to RDF data within documents published according to the Linked Data principles, SPARQL endpoints
are also a potential source of a great deal of Linked Data.
The execution of queries using languages such as SPARQL
can use utilise both of these types of data sources. In this
paper we present a hybrid approach to answering SPARQL
queries that makes use of both link traversal-based and distributed query processing-based approaches in order to combine query answering over the Web of Linked Data and
SPARQL endpoints respectively. The technique differs from
existing work in that link traversal and endpoint queries take
place in parallel without a static query plan. It is demonstrated how, using a set of heuristics and optimisation techniques, this can be effective when answering queries with
time constraints (incomplete answers are acceptable in order
to minimise execution time). An evaluation of the technique
is presented using the FedBench Linked Data queries with
query execution time limited to 10 seconds, with an analysis
of answers that can be provided within this time limit.",,
12/28/2020 19:36:53,2013,Discovering Meaningful Connections between Resources in the Web of Data,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-04.pdf,Laurens De Vocht,Ghent University,Belgium,Sam Coppens,Ghent University,Belgium,Ruben Verborgh,Ghent University,Belgium,Miel Vander Sande,Ghent University,Belgium,Erik Mannens,Ghent University,Belgium,Rik Van de Walle,Ghent University,Belgium,,,,"We will show that semantically annotated paths lead to discovering meaningful, non-trivial relations and connections
between multiple resources in large online datasets such as
the Web of Data. Graph algorithms have always been key
in pathfinding applications (e.g., navigation systems). They
make optimal use of available computation resources to find
paths in structured data. Applying these algorithms to
Linked Data can facilitate the resolving of complex queries
that involve the semantics of the relations between resources.
In this paper, we introduce a new approach for finding paths
in Linked Data that takes into account the meaning of the
connections and also deals with scalability. An efficient technique combining pre-processing and indexing of datasets
is used for finding paths between two resources in large
datasets within a couple of seconds. To demonstrate our
approach, we have implemented a testcase using the DBpedia dataset.
",,
12/28/2020 19:38:27,2013,Ranking Universities Using Linked Open Data,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-09.pdf,Rouzbeh Meymandpour,The University of Sydney, Australia,Joseph G. Davis,The University of Sydney, Australia,,,,,,,,,,,,,,,,"Ranking of universities represents a complex endeavor which
involves gathering, weighting, and analyzing diverse data.
Emerging semantic technologies enable the Web of Data, a giant
graph of interconnected information resources, also known as
Linked Data. A recent community effort, Linking Open Data
project, offers the possibility of accessing a large number of
semantically described and linked concepts in various domains. In
this paper, we propose a novel approach to take advantage of this
structured data in the domain of universities to develop proxy
measures of their relative standing for ranking purposes. Derived
from information theory, our approach of computing the
Information Content for universities and ranking them based on
these scores achieved results comparable to the international
ranking systems such as Shanghai Jiao Tong University, Times
Higher Education, and QS. The metric we developed can also be
used for innovative semantic applications in a range of domains
for entity ranking, information filtering, and multi-faceted
browsing.","Linked Data, Semantic Web, Entity Ranking, Web of Data, University Ranking, Linking Open Data, Partitioned Information Content, Informativeness Measurement.","H.1.1 [Models and Principles]: Systems and Information Theory ‚Äì information theory, value of information; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ‚Äì information filtering, retrieval models; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods ‚Äì semantic networks; J.1 [Computer Applications]: Administrative Data Processing ‚Äì education;"
12/28/2020 19:40:04,2013,Connected Media Experiences: interactive video using Linked Data on the Web,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-10.pdf,Lyndon Nixon,STI International, Austria,Matthias Bauer,STI International, Austria,Cristian Bara,Seekda GmbH, Austria,,,,,,,,,,,,,"This paper presents a set of tools and an extended framework with
API for enabling the dynamic enrichment of online video with
Web content via Linked Data. As audiovisual media is
increasingly transmitted online, new services deriving added value
from such material can be imagined. For example, combining it
with other material elsewhere on the Web which is related to it or
enhances it in a meaningful way, to the benefit of the owner of the
original content, the providers of the content enhancing it and the
end consumer who can access and interact with these new
services. Since the services are built around providing new
experiences through connecting different related media together,
we consider such services to be Connected Media Experiences
(ConnectME). This paper presents the ConnectME approach,
which is to annotate video with Linked Data concepts,
dynamically bind annotations to related Web content and then
provide a flexibly enriched video playout. Using Linked Data
allows us to decouple video annotations from hardcoded
enrichments, meaning a video can be annotated once but its
enrichment is always up to date. ","Hypervideo, clickable video, Web media, Linked Data, media linking, annotation, enrichment",
12/28/2020 19:44:19,2013,Describing Customizable Products on the Web of Data,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-11.pdf,François-Paul Servant,Renault, France,Edouard Chevalier,Renault, France,,,,,,,,,,,,,,,,"Exposing data about customizable products is a challenging
issue, because of the number of features and options customers can choose from, and of the intricate constraints between them. However, the configuration process, by which
the customer makes her choice, one step at a time, is a
graph traversal among partially defined products; that is
Linked Data browsing. This natural yet fruitful abstraction for product customization hides complexity from the
client agent, and allows corporations to publish descriptions
of their ranges of products, in their own terms. To open
these ranges to comparison, corporate vocabularies have to
be linked to known entities in the LOD cloud; the creation
of sharable thesauri is discussed.
","Linked Data, Structured eCommerce data, Configuration, GoodRelations, Automotive",H.4 [Information Systems Applications]: Miscellaneous
12/28/2020 19:48:55,2013,DDI-RDF Discovery Vocabulary: A Metadata Vocabulary for Documenting Research and Survey Data,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-12.pdf,Thomas Bosch,GESIS – Leibniz Institute for the Social Sciences, Germany,Richard Cyganiak,Digital Enterprise Research Institute, Ireland,Arofan Gregory,Open Data Foundation, US,Joachim Wackerow,GESIS – Leibniz Institute for the Social Sciences, Germany,,,,,,,,,,"The Data Documentation Initiative (DDI) is an acknowledged
international standard for the documentation and management of
data from the social, behavioral, and economic sciences.
Statistical domain experts, i.e. representatives of national
statistical institutes and national data archives, and Linked Open
Data community members have developed the DDI-RDF
Discovery Vocabulary ‚Äì based on a subset of the DDI - in order to
support the discovery of statistical data as well as metadata. This
vocabulary supports identifying programmatically the relevant
data sets for a specific research purpose.
","Linked Data, Ontology Design, statistical data, DDI-RDF",H.1 [Information Systems]: Models and Principles
12/28/2020 19:50:25,2013,Beyond Data: Building a Web of Needs,http://events.linkeddata.org/ldow2013/papers/ldow2013-paper-13.pdf,Florian Kleedorfer,Research Studios Austria, Austria,Christina Maria Busch,Research Studios Austria, Austria,,,,,,,,,,,,,,,,"The Web as related to commerce suffers from a fundamental asymmetry. While there is a great number of commercial
offers available, consumer needs are rarely represented explicitly. Thus, the most widely applied process of connecting
the prospective consumer of a resource with its supplier is
Web search. In Web search, the user needs are implicit,
driving the interaction, and therefore only the interaction
partners can try to deduce them. We present an approach
for a) publishing needs on the Web of Data and b) building
a protocol that allows decentralized matching of needs and
communication between need owners. Albeit inspired by the
analysis of marketplaces, the proposed framework allows for
a much broader range of social applications, such as collaborative problem solving, help organizing the sharing economy
or finding interesting people to meet.","semantic Web, ontology matching, instance matching, protocols, linked data",Information Systems [World Wide Web]: Web applications; Information Systems [World Wide Web]: Information System Applications‚ÄîCollaborative and social computing systems and tools
12/28/2020 19:53:06,2014,RML: A Generic Language for Integrated RDF Mappings of Heterogeneous Data,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_01.pdf,Anastasia Dimou,Multimedia Lab , Belgium,Miel Vander Sande,Multimedia Lab , Belgium,Pieter Colpaert,Multimedia Lab , Belgium,Ruben Verborgh,Multimedia Lab , Belgium,Erik Mannens,Multimedia Lab , Belgium,Rik Van de Walle,Multimedia Lab , Belgium,,,,"Despite the significant number of existing tools, incorporating data from multiple sources and different formats into the
Linked Open Data cloud remains complicated. No mapping
formalisation exists to define how to map such heterogeneous
sources into rdf in an integrated and interoperable fashion.
This paper introduces the rml mapping language, a generic
language based on an extension over rÔú≤rml, the wÔú≥c standard for mapping relational databases into rdf. Broadening
rÔú≤rml‚Äôs scope, the language becomes source-agnostic and
extensible, while facilitating the definition of mappings of
multiple heterogeneous sources. This leads to higher integrity
within datasets and richer interlinking among resources.",,
12/28/2020 19:55:19,2014,Knowledge Base Augmentation using Tabular Data,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_02.pdf,Yoones A. Sekhavat,University of Alberta, Canada,Francesco di Paolo,Roma Tre University , Italy,Denilson Barbosa,University of Alberta, Canada,Paolo Merialdo,Roma Tre University , Italy,,,,,,,,,,"Large linked data repositories have been built by leveraging semi-structured data in Wikipedia (e.g., DBpedia) and
through extracting information from natural language text
(e.g., YAGO). However, the Web contains many other vast
sources of linked data, such as structured HTML tables and
spreadsheets. Often, the semantics in such tables is hidden,
preventing one from extracting triples from them directly.
This paper describes a probabilistic method that augments
an existing knowledge base with facts from tabular data by
leveraging a Web text corpus and natural language patterns
associated with relations in the knowledge base. A preliminary evaluation shows high potential for this technique in
augmenting linked data repositories.",,
12/28/2020 19:57:51,2014,AIDA-light: High-Throughput Named-Entity Disambiguation,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_03.pdf,Dat Ba Nguyen,Max Planck Institute for Informatics, Germany,Johannes Hoffart,Max Planck Institute for Informatics, Germany,Martin Theobald,University of Antwerp, Belgium,Gerhard Weikum,Max Planck Institute for Informatics, Germany,,,,,,,,,,"To advance the Web of Linked Data, mapping ambiguous
names in structured and unstructured contents onto knowledge bases would be a vital asset. State-of-the-art methods
for Named Entity Disambiguation (NED) face major tradeoffs regarding efficiency/scalability vs. accuracy. Fast methods use relatively simple context features and avoid computationally expensive algorithms for joint inference. While
doing very well on prominent entities in clear input texts,
these methods achieve only moderate accuracy when fed
with difficult inputs. On the other hand, methods that
rely on rich context features and joint inference for mapping names onto entities pay the price of being much slower.
This paper presents AIDA-light which achieves high accuracy on difficult inputs while also being fast and scalable.
AIDA-light uses a novel kind of two-stage mapping algorithm. It first identifies a set of ‚Äúeasy‚Äù mentions with low
ambiguity and links them to entities in a very efficient manner. This stage also determines the thematic domain of the
input text as an important and novel kind of feature. The
second stage harnesses the high-confidence linkage for the
‚Äúeasy‚Äù mentions to establish more reliable contexts for the
disambiguation of the remaining mentions. Our experiments
with four different datasets demonstrates that the accuracy
of AIDA-light is competitive to the very best NED systems,
while its run-time is comparable to or better than the performance of the fastest systems.
","Entity Linking, Named Entity Disambiguation, Joint Inference, Scalability",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing
12/28/2020 20:08:27,2014,Web-Scale Querying through Linked Data Fragments,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_04.pdf,Ruben Verborgh,Ghent University, Belgium,Miel Vander Sande,Ghent University, Belgium,Pieter Colpaert,Ghent University, Belgium,Sam Coppens,Ghent University, Belgium,Erik Mannens,Ghent University, Belgium,Rik Van de Walle,Ghent University, Belgium,,,,"To unlock the full potential of Linked Data sources, we need flexible
ways to query them. Public sparql endpoints aim to fulfill that
need, but their availability is notoriously problematic. We therefore introduce Linked Data Fragments, a publishing method that
allows efficient offloading of query execution from servers to clients
through a lightweight partitioning strategy. It enables servers to
maintain availability rates as high as any regular http server, allowing querying to scale reliably to much larger numbers of clients.
This paper explains the core concepts behind Linked Data Fragments
and experimentally verifies their Web-level scalability, at the cost
of increased query times. We show how trading server-side query
execution for inexpensive data resources with relevant affordances
enables a new generation of intelligent clients.
","Linked Data, querying, availability, scalability, sparql",H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
12/28/2020 20:11:47,2014,DBpedia Viewer - An Integrative Interface for DBpedia Leveraging the DBpedia Service Eco System,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_05.pdf,Denis Lukovnikov,University of Leipzig, Germany,Claus Stadler,University of Leipzig, Germany,Dimitris Kontokostas,University of Leipzig, Germany,Sebastian Hellmann,University of Leipzig, Germany,Jens Lehmann,University of Leipzig, Germany,,,,,,,"With the growing interest in publishing data according to
the Linked Data principles, it becomes more important to
provide intuitive tools for users to view and interact with
resources. The characteristics of Linked Data pose several
challenges for user-friendly presentation of the information.
In this work, we present the DBpedia Viewer as one method
to address this problem. The DBpedia Viewer is the new
DBpedia Linked Data user interface, which makes DBpedia
data more accessible to non-experts while integrating the
DBpedia service eco system as well as external Linked Data
services.",,
12/28/2020 20:17:50,2014,Linked Data Query Wizard: A Novel Interface for Accessing SPARQL Endpoints,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_06.pdf,Patrick Hoefle,Know Center GmbH, Austria,Michael Granitzer,University of Passau, Germany,Eduardo Veas,Know Center GmbH, Austria,Christin Seifert,University of Passau, Germany,,,,,,,,,,"In an interconnected world, Linked Data is more important
than ever before. However, it is still quite difficult to access
this new wealth of semantic data directly without having
in-depth knowledge about SPARQL and related semantic
technologies. Also, most people are currently used to consuming data as 2-dimensional tables. Linked Data is by definition always a graph, and not that many people are used to
handle data in graph structures. Therefore we present the
Linked Data Query Wizard, a web-based tool for displaying,
accessing, filtering, exploring, and navigating Linked Data
stored in SPARQL endpoints. The main innovation of the
interface is that it turns the graph structure of Linked Data
into a tabular interface and provides easy-to-use interaction
possibilities by using metaphors and techniques from current
search engines and spreadsheet applications that regular web
users are already familiar with","Linked Data, User Interface, SPARQL, RDF Data Cube",
12/28/2020 20:19:47,2014,Programmable Analytics for Linked Open Data,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_07.pdf,Bo Hu,Fujitsu Laboratories, United Kingdom,Eduarda Mendes,Fujitsu Laboratories, United Kingdom,Emeric Viel,Fujitsu Laboratories, Japan,,,,,,,,,,,,,"LOD initiative has made a major impact on data provision.
Thus far, more than 800 datasets have been published, containing tens of billions of RDF triples. The sheer size of
data has not resulted in a significant increase of data consumption. We contend that a new programming paradigm
is necessary to simplify LOD data utilisation. This paper
reports an early phase development towards programmable
web of LOD data. We propose to tap into a distributed
computing environment underpinning the popular statistical
toolkit R. Where possible, native R operators and functions
are used in our approach so as to lower the learning curve.
The crux of our future work lies in the full implementation
and evaluation.
","Linked Open Data, RDF, R, Programmability",H.4 [Information Systems Applications]: Miscellaneous; D.2.12 [Interoperability]: Data mapping
12/28/2020 20:23:41,2014,Will Linked Data Benefit from Inverse Link Traversal? (Position Proposal),http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_08.pdf,Stefan Scheglmann,University of Koblenz, Germany,Ansgar Scherp,Kiel University, Germany,,,,,,,,,,,,,,,,"Query execution using link-traversal is a promising approach
for retrieving and accessing data on the web. However, this
approach finds its limitation when it comes to query patterns
such as ?s rdf:type ex:Employee, where one does not know
the subject URI. Such queries are quite useful for different
application needs. In this paper, we conduct an empirical
analysis on the use of such patterns in SPARQL query logs.
We present different solution approaches to extend the current Linked Open Data principles with the ability for inverse
link traversal. We discuss the advantages and disadvantages
of the different approaches.",Querying of Linked Open Data; Link Traversal,H.1 [Information Systems Applications]: Models and Principles; H.4 [Information Systems Applications]: Miscellaneous
12/28/2020 20:25:12,2014,daQ: an Ontology for Dataset Quality Information,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_09.pdf,Jeremy Debattista,University of Bonn, Germany,Christoph Lange,University of Bonn, Germany,Sören Auer,University of Bonn, Germany,,,,,,,,,,,,,"Data quality is commonly defined as fitness for use. The problem
of identifying the quality of data is faced by many data consumers.
To make the task of finding good quality datasets more efficient, we
introduce the Dataset Quality Ontology (daQ). The daQ is a lightweight, extensible vocabulary for attaching the results of quality
benchmarking of a linked open dataset to that dataset. We discuss the design considerations, give examples for extending daQ
by custom quality metrics, and present use cases such as browsing
datasets by quality. We also discuss how tools can use the daQ to
enable consumers find the right dataset for use.
",,"The Web of Data [Vocabularies, taxonomies and schemas for the web of data]"
12/28/2020 20:26:20,2014,Publishing L2TAP Logs to Facilitate Transparency and Accountability,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_10.pdf,Reza Samavi,University of Toronto, Canada,Mariano P. Consens,University of Toronto, Canada,,,,,,,,,,,,,,,,"We propose publishing L2TAP privacy logs to facilitate privacy auditing tasks that involve multiple auditors, an increasingly common requirement in the context of social computing
and big data driven science. Our proposal utilizes two ontologies, L2TAP and SCIP, designed for deployment in a
Linked Data environment. L2TAP provides provenance enabled logging of events. SCIP synthesizes contextual integrity
concepts to express key privacy-related semantics associated
with log events. We describe SPARQL query-based solutions for privacy log construction, obligation derivation, and
compliance checking. The solutions facilitate accountability
and transparency among participants (privacy auditors in
particular).",,
12/28/2020 20:38:04,2014,Weaving the Web(VTT) of Data,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_11.pdf,Thomas Steiner,Université de Lyon, France,Hannes Mühleisen,Database Architectures Group, The Netherlands,Ruben Verborgh,Ghent University,Belgium,Pierre-Antoine Champin,Université de Lyon, France,Benoît Encelle,Université de Lyon, France,Yannick Prié,Université de Nantes, France,,,,"Video has become a first class citizen on the Web with broad
support in all common Web browsers. Where with structured mark-up on webpages we have made the vision of the
Web of Data a reality, in this paper, we propose a new vision that we name the Web(VTT) of Data, alongside with
concrete steps to realize this vision. It is based on the
evolving standards WebVTT for adding timed text tracks
to videos and JSON-LD, a JSON-based format to serialize Linked Data. Just like the Web of Data that is based
on the relationships among structured data, the Web(VTT)
of Data is based on relationships among videos based on
WebVTT files, which we use as Web-native spatiotemporal
Linked Data containers with JSON-LD payloads. In a first
step, we provide necessary background information on the
technologies we use. In a second step, we perform a largescale analysis of the 148 terabyte size Common Crawl corpus
in order to get a better understanding of the status quo of
Web video deployment and address the challenge of integrating the detected videos in the Common Crawl corpus into
the Web(VTT) of Data. In a third step, we open-source
an online video annotation creation and consumption tool,
targeted at videos not contained in the Common Crawl corpus and for integrating future video creations, allowing for
weaving the Web(VTT) of Data tighter, video by video.","JSON-LD, Linked Data, media fragments, SemanticWeb, video annotation, Web of Data, WebVTT, Web(VTT) of Da",H.5.1 [Multimedia Information Systems]: Video
12/28/2020 21:59:26,2014,Social Web Meets Sensor Web: From User-Generated Content to Linked Crowdsourced Observation Data,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_12.pdf,Dong–Po Deng,Academia Sinica, Taiwan,Guan–Shuo Mai,Academia Sinica, Taiwan,Tyng–Ruey Chuang,Academia Sinica, Taiwan,Rob Lemmens,University of Twente, The Netherlands,Kwang–Tsao Shao,Academia Sinica, Taiwan,,,,,,,"The reach of dominating social media like Facebook and
Twitter in the current population is enormous, and these
media have long been leveraged for diverse applications. In
particular, for some citizen science projects, existing social
media increasingly become platforms on which participants
interact and contribute. These user contributions, often
termed User-Generated Content (UGC), can be a mix bag
of posts, comments, images, and other media. We report
in this paper a work-in-progress in formalizing user contributions from a large Facebook group (more than 4,000
users) established for biodiversity observation. A major
part of our work is to extract structured datasets with welldefined semantics from unstructured UGC collections. We
use common vocabularies from Darwin Core (DwC), Friendof-a-friend (FOAF), Semantically-Interlinked Online Communities (SIOC), Semantic Sensor Network (SSN), among others, to formalize the extracted datasets, hence, make
them readily linkable. A nice consequence of this approach
is that a multi-faceted browser can be quickly built to explore biodiversity information in large collections of UGC.","Citizen Science, Crowdsourcing, Facebook, GeoSPARQL, Linked Data, Sensor Network, User-Generated Content (UGC)",H.3.5 [Online Information System]: [Web-based services]; H.5.3 [Group and Organization Interfaces]: [Web-based Interaction]; I.2.4 [Knowledge Representation Formalisms and Methods]: Semantic Networks
12/28/2020 22:02:32,2014,Application of the Linked Data Visualization Model on Real World Data from the Czech LOD Cloud,http://events.linkeddata.org/ldow2014/papers/ldow2014_paper_13.pdf,Jakub Klímek,Czech Technical University in Prague, Czechia,Jiˇrí Helmich,Charles University, Czechia,Martin Necask_,Charles University, Czechia,,,,,,,,,,,,,"In the recent years the Linked Open Data phenomenon has
gained a substantial traction. This has lead to a vast amount
of data being available on the Web in what is known as the
LOD cloud. While the potential of this linked data space
is huge, it fails to reach the non-expert users so far. At the
same time there is even larger amount of data that is so
far not open yet, often because its owners are not convinced
of its usefulness. In this paper we refine our Linked Data
Visualization Model (LDVM) and show its application via
its implementation Payola. On a real-world scenario built
on real-world Linked Open Data created from Czech open
data sources we show how end-user friendly visualizations
can be easily achieved. Our first goal is to show that using
Payola, existing Linked Open Data can be easily mashed
up and visualized using an extensible library of analyzers,
transformers and visualizers. Our second goal is to give potential publishers of (Linked) Open Data a proof that simply
by publishing their data in a right way can bring them powerful visualizations at virtually no additional cost.","Linked Data, Visualization, Semantic Web","H.5.2 [User interfaces]: GUIs, Interaction styles; H.3.5 [Online Information Services]: Data sharing; H.3.5 [Online Information Services]: Web-based services"
12/28/2020 22:04:48,2015,Fixing the Domain and Range of Properties in Linked Data by Context Disambiguation,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_01.pdf,Alberto Tonon,University of Fribourg, Switzerland,Michele Catasta,Lausanne, Switzerland,Gianluca Demartini,University of Sheffield, United Kingdom,Philippe Cudré-Mauroux,University of Fribourg, Switzerland,,,,,,,,,,"The amount of Linked Open Data available on the Web is
rapidly growing. The quality of the provided data, however,
is generally-speaking not fundamentally improving, hampering its wide-scale deployment for many real-world applications. A key data quality aspect for Linked Open Data can
be expressed in terms of its adherence to an underlying welldefined schema or ontology, which serves both as a documentation for the end-users as well as a fixed reference for
automated processing over the data. In this paper, we first
report on an analysis of the schema adherence of domains
and ranges for Linked Open Data. We then propose new
techniques to improve the correctness of domains and ranges
by i) identifying the cases in which a property is used in the
data with several different semantics, and ii) resolving them
by updating the underlying schema and/or by modifying the
data without compromising its retro-compatibility. We experimentally show the validity of our methods through an
empirical evaluation over DBpedia by creating expert judgements of the proposed fixes over a sample of the data.
","Linked Open Data, Schema adherence, Data quality",H.4.m [Information Systems]: Miscellaneous
12/28/2020 22:06:00,2015,Rule Mining for Semantifying Wikilinks,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_02.pdf,Luis Galárraga,Télécom ParisTech, France,Danai Symeonidou,Télécom ParisTech, France,Jean-Claude Moissinac,Télécom ParisTech, France,,,,,,,,,,,,,"Wikipedia-centric Knowledge Bases (KBs) such as YAGO
and DBpedia store the hyperlinks between articles in Wikipedia using wikilink relations. While wikilinks are signals of
semantic connection between entities, the meaning of such
connection is most of the times unknown to KBs, e.g., for
89% of wikilinks in DBpedia no other relation between the
entities is known. The task of discovering the exact relations
that hold between the endpoints of a wikilink is called wikilink semantification. In this paper, we apply rule mining
techniques on the already semantified wikilinks to propose
relations for the unsemantified wikilinks in a subset of DBpedia. By mining highly supported and confident logical
rules from KBs, we can semantify wikilinks with very high
precision.",,
12/28/2020 22:07:50,2015,Towards Automatic Topical Classification of LOD Datasets,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_03.pdf,Robert Meusel,University of Mannheim, Germany,Blerina Spahiu,University of Milan Bicocca, Italy,Christian Bizer,University of Mannheim, Germany,Heiko Paulheim,University of Mannheim, Germany,,,,,,,,,,"The datasets that are part of the Linking Open Data cloud
diagramm (LOD cloud) are classified into the following topical categories: media, government, publications, life sciences, geographic, social networking, user-generated content, and cross-domain. The topical categories were manually assigned to the datasets. In this paper, we investigate to
which extent the topical classification of new LOD datasets
can be automated using machine learning techniques and the
existing annotations as supervision. We conducted experiments with different classification techniques and different
feature sets. The best classification technique/feature set
combination reaches an accuracy of 81.62% on the task of
assigning one out of the eight classes to a given LOD dataset.
A deeper inspection of the classification errors reveals problems with the manual classification of datasets in the current
LOD cloud.","Linked Open Data, Topic Detection, Data Space Profiling",
12/28/2020 22:09:37,2015,Interlinking: Performance Assessment of User Evaluation vs. Supervised Learning Approaches,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_04.pdf,Mofeed Hassan,University of Leipzig, Germany,Jens Lehmann,University of Leipzig, Germany,Axel-Cyrille Ngonga Ngomo,University of Leipzig, Germany,,,,,,,,,,,,,"Interlinking knowledge bases are widely recognized as an important, but challenging problem. A significant amount of
research has been undertaken to provide solutions to this
problem with varying degrees of automation and user involvement. In this paper, we present a two-staged experiment for the creation of gold standards that act as benchmarks for several interlinking algorithms. In the first stage
the gold standards are generated through manual validation
process highlighting the role of users. Using the gold standards obtained from this stage, we assess the performance
of human evaluators in addition to supervised interlinking
algorithms. We evaluate our approach on several data interlinking tasks with respect to precision, recall and F-measure.
Additionally we perform a qualitative analysis on the types
of errors made by humans and machines.
","Interlinking, Links validation, Gold standard, Manual validation, Performance evaluation",H.4 [LINK Discovery]
12/28/2020 22:12:10,2015,DBpedia Atlas: Mapping the Uncharted Lands of Linked Data,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_05.pdf,Fabio Valsecchi,Institute of Informatics and Telematics, Italy,Matteo Abrate,Institute of Informatics and Telematics, Italy,Clara Bacciu,Institute of Informatics and Telematics, Italy,Maurizio Tesconi Institute of Informatics and,Institute of Informatics and Telematics, Italy,Andrea Marchetti,Institute of Informatics and Telematics, Italy,,,,,,,"In the last few years, Linked Open Data sources have extremely increased in number. Despite their enormous potential, it is really hard to find effective and efficient ways
for navigating and exploring them, mainly because of complexity and volume issues. In fact, application developers,
students and researchers that are not experts in Semantic
Web technologies often lose themselves in the intricacies of
the Web of Data. We propose to address this problem by
providing users with a map-like visualization that acts as an
entry point for the exploration of a dataset. To this end, we
adapt a spatialization approach, based on cartographic and
information visualisation techniques, to make it suitable for
Linked Data sets with a hierarchical ontological structure.
Finally, we apply our method on DBpedia, implementing
and testing a prototype web application that shows a comprehensive and organic representation of the more than 4
million instances defined by the dataset.","Linked Data, Information Visualisation, Cartography",H.5.0 [Information Interfaces and Presentation]: General
12/28/2020 22:13:29,2015,Keyword-Based Navigation and Search over the Linked Data Web,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_06.pdf,Luca Matteis,Sapienza University of Rome, Italy,Aidan Hogan,University of Chile, Chile,Roberto Navigli,Sapienza University of Rome, Italy,,,,,,,,,,,,,"Keyword search approaches over RDF graphs have proven
intuitive for users. However, these approaches rely on local
copies of RDF graphs. In this paper, we present an algorithm that uses RDF keyword search methodologies to find
information in the live Linked Data web rather than against
local indexes. Users navigate between documents by specifying keywords that are matched against triples. Navigation
is performed through a pipeline which streams results to
users as soon as they are found. Keyword search is assisted
through the resolution of predicate URIs. We evaluate our
methodology by converting several natural language questions into lists of keywords and seed URIs. For each question
we measured how quickly and how many triples appeared in
the output stream of each step of the pipeline. Results show
that relevant triples are streamed back to users in less than
5 seconds on average. We think that this approach can help
people analyze and explore various Linked Datasets in a follow your nose fashion by simply typing keywords.",,
12/28/2020 22:16:52,2015,Uduvudu: a Graph-Aware and Adaptive UI Engine for Linked Data,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_07.pdf,Michael Luggen,University of Fribourg, Switzerland,Adrian Gschwend,Bern University of Applied Sciences, Switzerland,Bernhard Anrig,University of Fribourg, Switzerland,Philippe Cudré-Mauroux,Bern University of Applied Sciences, Switzerland,,,,,,,,,,"Creating good User Interfaces (UIs) to render Linked Data
visually is a complex task, often involving both UI and
Linked Data specialists. The resulting solutions are typically application-dependent and difficult to adapt or reuse
in a different context. To tackle this problem, we propose
Uduvudu, a flexible, open-source engine to visualize Linked
Data. Our engine is built in JavaScript and runs in the
browser natively. Non-specialist users can use Uduvudu to
describe recurring subgraph patterns occurring in their data.
They can then flexibly and automatically extract, transform,
and visually render such patterns in multiple ways depending of the usage context. Uduvudu is intuitive, flexible, and
efficient and makes it possible to jump-start the development
of complex user interfaces based on Linked Data without the
need of data specialists.
","Templating, Visualization, User Interface, Development Process, Linked Data, RDF",
12/28/2020 22:24:32,2015,Use Cases for Linked Data Visualization Model,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_08.pdf,Jakub Klímek,Czech Technical University in Prague, Czechia,Jiˇrí Helmich,Charles University, Czechia,Martin Necask_,Czech Technical University in Prague, Czechia,,,,,,,,,,,,,"There is a vast amount of Linked Data on the web spread
across a large number of datasets. One of the visions behind Linked Data is that the published data is conveniently
reusable by others. This, however, depends on many details
such as conformance of the data with commonly used vocabularies and adherence to best practices for data modeling.
Therefore, when an expert wants to reuse existing datasets,
he still needs to analyze them to discover how the data is
modeled and what it actually contains. This may include
analysis of what entities are there, how are they linked to
other entities, which properties from which vocabularies are
used, etc. What is missing is a convenient and fast way of
seeing what could be usable in the chosen unknown dataset
without reading through its RDF serialization. In this paper
we describe use cases based on this problem and their realization using our Linked Data Visualization Model (LDVM)
and its new implementation. LDVM is a formal base that
exploits the Linked Data principles to ensure interoperability
and compatibility of compliant analytic and visualization
components. We demonstrate the use cases on examples
from the Czech Linked Open Data cloud.","Linked Data, RDF, visualization, discovery","H.5.2 [User interfaces]: GUIs, Interaction styles; H.3.5 [Online Information Services]: Data sharing; H.3.5 [Online Information Services]: Web-based services"
12/28/2020 22:25:45,2015,Simplified RDB2RDF Mapping,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_09.pdf,Claus Stadler,University of Leipzig, Germany,Jörg Unbehauen,University of Leipzig, Germany,Patrick Westphal,University of Leipzig, Germany,Mohamed Sherif,University of Leipzig, Germany,Jens Lehmann,University of Leipzig, Germany,,,,,,,"The combination of the advantages of widely used relational
databases and semantic technologies has attracted significant research over the past decade. In particular, mapping
languages for the conversion of databases to RDF knowledge
bases have been developed and standardized in the form of
R2RML. In this article, we first review those mapping languages and then devise work towards a unified formal model
for them. Based on this, we present the Sparqlification Mapping Language (SML), which provides an intuitive way to
declare mappings based on SQL VIEWS and SPARQL construct queries. We show that SML has the same expressivity
as R2RML by enumerating the language features and show
the correspondences, and we outline how one syntax can be
converted into the other. A conducted user study for this
paper juxtaposing SML and R2RML provides evidence that
SML is a more compact syntax which is easier to understand
and read and thus lowers the barrier to offer SPARQL access
to relational databases.",,
12/28/2020 22:27:32,2015,ESCO: Towards a Semantic Web for the European Labor Market,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_10.pdf,Johan De Smedt,Havenkant 38,Belgium,Martin le Vrang,Joseph II straat 27, Belgium,Agis Papantoniou,Havenkant 38, Belgium,,,,,,,,,,,,,"The Semantic Web has a huge potential when it is used to
organize market processes. The labour market is an excellent
example where it can add value. By enhancing communication
between employers and job seekers in the digital age, market
processes become more efficient and more people can find the
right jobs for them. The multilingual classification of European
Skills, Competences, Qualifications and Occupations (ESCO) is a
central building block for an ecosystem of semantic assets on the
labour market. In this paper we explain how the ESCO data model
was designed, building on the Simple Knowledge Organization
System. We further explain how new versions of the datasets are
published and how they can be used by applications. Finally, we
summarize the next steps in the on-going work on ESCO. ","ESCO, Job Labour Market, Linked Open Data, Open Standards, Knowledge Organization System, RDF, DCAT",H.3.1 [Content Analysis and Indexing]; H.3.3 [Information Search and Retrieval]; I.2.4 [Knowledge Representation Formalisms and Methods]; H.3.5 [Online Information systems]; J.1 [Administrative Data Processing].
12/28/2020 22:29:35,2015,Bringing Agility into Linked Data Development: An Industrial Use Case in Logistics Domain,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_11.pdf,Pinar Gocebe,Ege University, Turkey,Oguz Dikenelli,Ege University, Turkey,Nuri Umut Kose,BIMAR Information Technologies, Turkey,,,,,,,,,,,,,"Abstract
Logistics is a complex industry where many different types of companies collaborate in order to transport containers to the last point.
One of the most important problem in logistics domain is observation and monitoring of container life cycle where each step of
the container transportation may be performed by different company. Thus, observing and monitoring of the container‚Äôs life cycle in real time become a challenging engineering task. In this research, Linked Data development infrastructure has been used to
implement dynamic container observation and monitoring system
for ARKAS company which is the leading logistics company in
Turkey. During the development of the system, it has been observed
that agile practices like feature/story oriented development, test first
development and usage of Agile Architecture approach improves
the product and project management quality. So, a new methodology has been proposed based on these practices for Linked Data
development.","Linked Data Development Methodology, Agile Analytics, Agile Architecture",
12/28/2020 22:42:30,2015,Normalizing Resource Identifiers using Lexicons in the Global Change Information System,http://events.linkeddata.org/ldow2015/papers/ldow2015_paper_12.pdf,Brian Duggan,US Global Change Research Program, US,Curt Tilmes,NASA Goddard Space Flight Center, US,Steven Aulenbach,US Global Change Research Program, US,Robert E. Wolfe,US Global Change Research Program, US,Justin C. Goldstein,US Global Change Research Program, US,Gerald Manipon,NASA Goddard Space Flight Center, US,,,,"Earth Science informatics involves collaboration between multiple groups of people with diverse specializations and goals,
often using variations in terminology to refer to common resources. The uniformity of the resource identifiers often does
not cross organizational boundaries. Because of this, permanent, widely used, unambiguous identifiers for resources are
elusive. We examine real world cases of changing and inconsistent identifiers which inherently work against persistence
and uniformity. We also present a solution which mediates
factors in these situations; namely the creation of lexicons:
mappings of sets of terms to URIs which are curated within
the Global Change Information System (GCIS).
We discuss aspects of the GCIS which facilitate the use
of lexicons: an information model which disambiguates resources, a RESTful API which provides metadata through
content-negotiation, and a strategy for long term curation of
URIs, including mechanisms for handling changes to URIs
and variations in terms used by different communities while
providing persistent URIs and preserving relationships between resources.
We provide working definitions of terms, contexts, and lexicons, and relate them to the practical challenges of disambiguation and curation. We also discuss the mechanisms employed and architecture of the GCIS, and how these choices
facilitate representation of persistent identifiers and mappings of them to identifiers used colloquially within various
earth science communities of practice.","Linked Data, URI, Co-reference",
12/28/2020 22:44:26,2016,Requirements on Linked Data Consumption Platform,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_01.pdf,Jakub Klímek,Charles University, Czechia,Petr _koda,Charles University, Czechia,Martin Necask_,Charles University, Czechia,,,,,,,,,,,,,"The publication of data as Linked Open Data (LOD) gains
traction. There are lots of different datasets published, more
vocabularies are becoming W3C Recommendations and with
the introduction of DCAT-AP v1.1 and the emergence of the
European data portal and a multitude of national open data
portals, lots of datasets are discoverable and accessible using
their DCAT-AP metadata in RDF. Yet, the consumption
of LOD is lacking in comfort and availability of tools that
would exploit the benefits of LOD and allow users to discover,
access, integrate and reuse LOD easily, as promised by the
promoters of LOD and supposedly paid by the additional
effort put into the 5-star data publication by the publishers.
Compared to the consumption of 3-star CSV and XML files,
the consumption of LOD is still quite complicated and the
LOD benefits are not exploited enough nor visible enough
to justify the effort for many publishers. In this paper we
identify 40 requirements which a Linked Data Consumption
Platform (LDCP) should satisfy in order to be able to exploit the LOD benefits in a way that would ease the LOD
consumption and justify the additional effort put into LOD
publication. We survey 8 relevant and currently available
tools based on their coverage of the identified requirements.
","Linked Data, RDF, consumption, discovery, visualization",H.3.5 [Online Information Services]: Data sharing; H.3.5 [Online Information Services]: Web-based services
12/28/2020 22:46:16,2016,Structured Feedback: A Distributed Protocol for Feedback and Patches on the Web of Data,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_02.pdf,Natanael Arndt,Leipzig University, Germany,Kurt Junghanns,Leipzig University, Germany,Roy Meissner,Leipzig University, Germany,Philipp Frischmuth,Leipzig University, Germany,Norman Radtke,Leipzig University, Germany,Marvin Frommhold,Leipzig University, Germany,Michael Martin,Leipzig University,Germany,"The World Wide Web is an infrastructure to publish and retrieve information through web resources. It evolved from a
static Web 1.0 to a multimodal and interactive communication and information space which is used to collaboratively
contribute and discuss web resources, which is better known
as Web 2.0. The evolution into a Semantic Web (Web 3.0)
proceeds. One of its remarkable advantages is the decentralized and interlinked data composition. Hence, in contrast
to its data distribution, workflows and technologies for decentralized collaborative contribution are missing. In this
paper we propose the Structured Feedback protocol as an
interactive addition to the Web of Data. It offers support
for users to contribute to the evolution of web resources, by
providing structured data artifacts as patches for web resources, as well as simple plain text comments. Based on
this approach it enables crowd-supported quality assessment
and web data cleansing processes in an ad-hoc fashion most
web users are familiar with.","linked data, dssn, semantic pingback, pubsubhubub, feedback, distributed semantic services, quality, crowd sourcing",
12/28/2020 22:48:11,2016,Towards a Collaborative Process Platform: Publishing Processes according to the Linked Data Principles,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_03.pdf,Tobias Weller,"AIFB Institute, KIT", Germany,Maria Maleshkova,"AIFB Institute, KIT", Germany,,,,,,,,,,,,,,,,"Research in the area of process modeling and analysis has a
long-established tradition. Process modeling is among others used in the medical domain to define an ideal workflow in
order to ensure an efficient treatment of patients. These processes are often defined and maintained by multiple persons.
Furthermore, multiple persons are interested in these defined
processes to compare them with own defined processes for
improvements purposes. Current solutions provide tools to
model processes locally and export them in standard formats
in order to exchange them. Besides, there are some collaboration tools available to model processes collaboratively and
see changes dynamically. However, these solutions do not
publish the data according to the Linked Data principles.
Enriching processes with semantic information is useful in
order to perform enhanced analysis. However, different users
can only provide particular meta-information on same process steps. To address these problems we 1) developed an intuitive, open-source extension for Semantic MediaWiki that
supports the graphical modeling of processes and stores the
information in a structured way; 2) enable to enrich the processes with semantics from ontologies and knowledge graphs
with references to external data sources 3) provide adapted
views on meta-information in order to not overwhelm users
with unnecessary information.","Business Process Model and Notation, Semantic MediaWiki, Linked Data principles, Collaborative Platform, User Roles",
12/28/2020 22:49:39,2016,Automated Metadata Generation for Linked Data Generation and Publishing Workflows,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_04.pdf,Anastasia Dimou,Ghent University,Belgium,Tom De Nies,Ghent University,Belgium,Ruben Verborgh,Ghent University,Belgium,Erik Mannens,Ghent University,Belgium,Rik Van de Walle,Ghent University,Belgium,,,,,,,"Provenance and other metadata are essential for determining ownership and trust. Nevertheless, no systematic approaches were introduced so far in the Linked Data publishing workflow to capture them. Defining such metadata
remained independent of the rdf data generation and publishing. In most cases, metadata is manually defined by the
data publishers (person-agents), rather than produced by
the involved applications (software-agents). Moreover, the
generated rdf data and the published one are considered to
be one and the same, which is not always the case, leading to
pure, condense and often seductive information. This paper
introduces an approach that relies on declarative descriptions of (i) mapping rules, specifying how the rdf data is
generated, and of (ii) raw data access interfaces to automatically and incrementally generate provenance and metadata
information. This way, it is assured that the metadata information is accurate, consistent and complete.",,
12/28/2020 22:56:02,2016,Complex Schema Mapping and Linking Data: Beyond Binary Predicates,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_07.pdf,Jacobo Rouces,Aalborg University, Denmark,Gerard de Melo,Tsinghua University, China,Katja Hose,Aalborg University, Denmark,,,,,,,,,,,,,"Currently, datasets in the Linked Open Data (LOD) cloud
are mostly connected by properties such as owl:sameAs,
rdfs:subClassOf, or owl:equivalentProperty. These properties either link pairs of entities that are equivalent or express some other binary relationship such as subsumption.
In many cases, however, this is not sufficient to link all types
of equivalent knowledge. Often, a relationship exists between
an entity in one dataset and what is represented by a complex
pattern in another, or between two complex patterns. In
this paper, we present a method for linking datasets that
is expressive enough to support these cases. It consists of
integration rules between arbitrary datasets and a mediated
schema. We also present and evaluate a method to create
these integration rules automatically.","Linked Open Data, Schema matching, Heterogeneous knowledge, Linguistic frames, Data integration",H.4 [Information Systems Applications]: Miscellaneous; H.1.0 [Information Systems]: Models and Principles‚Äî General
12/28/2020 22:59:29,2016,Discovering Spatial and Temporal Links among RDF Data,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_08.pdf,Panayiotis Smeros,EPFL, Switzerland,Manolis Koubarakis,National and Kapodistrian University of Athens, Greece,,,,,,,,,,,,,,,,"Link Discovery is a new research area of the Semantic Web
which studies the problem of finding semantically related
entities lying in different knowledge bases. This area has
become more crucial recently, as the volume of the available
Linked Data on the web has been increasing considerably.
Although many link discovery tools have been developed,
none of them takes into consideration the discovery of spatial or temporal relations, leaving datasets with such characteristics weakly interlinked and therefore disallowing the
exploitation of the rich information they provide.
In this paper, we propose new methods for Spatial and
Temporal Link Discovery and provide the first implementation of our techniques based on the well-known framework
Silk. Silk, enhanced with the new features, allows data
publishers to generate a wide variety of spatial, temporal
and spatiotemporal relations between their data and other
Linked Open Data, dealing effectively with the common heterogeneity issues of such data. Furthermore, we experimentally evaluate our implementation by using it in a real-world
scenario and demonstrate that it discovers accurately all the
existing links in a time efficient and scalable way.
","Spatial and Temporal Link Discovery, Semantic Web, Linked Data",
12/28/2020 23:04:12,2016,Assessing Quantity and Quality of Links Between Linked Data Datasets,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_09.pdf,Ciro Baron Neto,Leipzig University, Germany,Dimitris Kontokostas,Leipzig University, Germany,Sebastian Hellmann,Leipzig University, Germany,Kay Müller,Leipzig University, Germany,Martin Brümmer,Leipzig University, Germany,,,,,,,"The Linked Data Web is growing and it becomes increasingly
necessary to analyze the relationship between datasets to exploit its full value. LOD datasets can range from datasets
with low cohesion ‚Äì containing data from different Fully
Qualified Domain Names (FQDN) and namespaces ‚Äì to highly
cohesive datasets. This paper evaluates the quantity and
quality of links between distributions, datasets and ontologies categorizing and defining different types of links. We
streamed and indexed 2.5 billion triples and extracted 0.5
billion links using probabilistic data structures. Our results
show the analysis of datasets w.r.t. valid links, dead links,
and number of namespaces described by distributions and
datasets. Our results indicate that 7.9% of the links we indexed and verified are actually dead.
","Linked Open Data, Linksets, Dead Links, RDF",
12/28/2020 23:06:07,2016,Improving Link Specifications using Context-Aware Information,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_10.pdf,Andrea Cimmino,University of Seville, Spain,Carlos R. Rivero,Rochester Institute of Technology, US,David Ruiz,University of Seville, Spain,,,,,,,,,,,,,"There is an increasing interest in publishing data using the
Linked Open Data philosophy. To link the RDF datasets,
a link discovery task is performed to generate owl:sameAs
links. There are two ways to perform this task: by means
of a classifier or a link specification; we focus in the latter approach. Current link specification techniques only use
the data properties of the instances that they are linking,
and they do not take the context information into account.
In this paper, we present a proposal that aims to generate context-aware link specifications to improve the regular
link specifications, increasing the effectiveness of the results
in several real-world scenarios where the context is crucial.
Our context-aware link specifications are independent from
similarity functions, transformations or aggregations. We
have evaluated our proposal using two real-world scenarios
in which we improve precision and recall with respect to
regular link specifications in 23% and 58%, respectively.","Linked Data, Link Discovery, Link Specification, ContexAware Link Specification",
12/28/2020 23:08:40,2016,Publish and Subscribe for RDF in Enterprise Value Networks,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_05.pdf,Marvin Frommhold,University of Leipzig, Germany,Natanael Arndt,University of Leipzig, Germany,Sebastian Tramp,eccenca GmbH, Germany,Niklas Petersen,University of Bonn, Germany,,,,,,,,,,"Sharing information securely between business partners and
managing large supply chains efficiently will be a crucial
competitive advantage for enterprises in the near future. In
this paper, we present a concept that allows for building
value networks between business partners in a distributed
manner. Companies are able to publish Linked Data which
participants of the network can clone and subscribe to. Subscribers get notified as soon as new information becomes
available. This provides a technical infrastructure for business communication acts such as supply chain communication or master data management. In addition to the conceptual analysis, we provide an implementation enabling companies to create such dynamic semantic value networks.
","Access control, Change propagation, Linked data, Publish and subscribe, RDF, Replication",
12/28/2020 23:10:34,2016,"Annalist: A practical tool for creating, managing and sharing evolving linked data",http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_11.pdf,Graham Klyne,University of Oxford, United Kingdom,Cerys Willoughby,University of Southampton, United Kingdom,Kevin Page,University of Oxford, United Kingdom,,,,,,,,,,,,,"Annalist is a software system for individuals and small groups
to reap the benefits of using RDF linked data, supporting
them to easily create data that participates in a wider web of
linked data. It presents a flexible web interface for creating,
editing and browsing evolvable data, without requiring the
user to be familiar with minutiae of the RDF model or
syntax, or to perform any programming, HTML coding or
prior configuration.
Development of Annalist was motivated by data capture
and sharing concerns in a small bioinformatics research group,
and customized personal information management. Requirements centre particularly on achieving low activation energy
for simple tasks, flexibility to add structural details as data
is collected, access-controlled sharing, and ability to connect
private data with public data on the web. It is designed as a
web server application, presenting an interface for defining
data structure and managing data. Data is stored as text
files that are amenable to access by existing software, with
the intent that a range of applications may be used in concert
to gather, manage and publish data.
During its development, Annalist has been used in a range
of applications, which have informed decisions about its
design and proven its flexibility and robustness in use. It has
been particularly effective in exploring and rapid prototyping
designs for linked data on the web, covering science and
humanities research, creative art and personal information.
","Semantic Web, Linked data, Data management",
12/28/2020 23:13:41,2016,"KnowledgeWiki: An OpenSource Tool for Creating Community-Curated Vocabulary, with a Use Case in Materials Science",http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_12.pdf,Nishita Jaykumar,Wright State University, US,PavanKalyan Yallamelli,Wright State University, US,Vinh Nguyen,Wright State University, US,Sarasi Lalithsena,Wright State University, US,Krishnaprasad Thirunarayan,Wright State University, US,Amit Sheth,Wright State University, US,Clare Paul,Wright State University,US,"Resource Description Framework (RDF) datasets can be
created by transforming structured databases, extracting
the triples from semi-structured and unstructured sources,
crowd-sourcing, or by integrating the existing datasets. The
reliability and quality of these datasets can be improved by
the participation of domain experts via a special purpose
tool or a crowd-sourced application. Wikidata and Semantic MediaWiki are platforms which facilitate this kind of
crowd-sourced data curation.
We present our system, KnowledgeWiki, which is built
upon the existing Semantic MediaWiki. We develop a novel
extension by adopting the singleton property data model in
our KnowledgeWiki. This extension allows various kinds of
metadata about the RDF triples to be created in the Wiki.
We combine this extension with other extensions such as semantic forms to provide a user-friendly, Wiki-like interface
for domain experts with no prior technical expertise to easily
curate data. We also present our new enhancement to Semantic Mediawiki, which facilitates importing existing RDF
datasets into the wiki-based curating platform based on the
singleton property approach, that preserves the provenance
of individual triples. We also describe how it is being used
by the materials science community to create and curate
consolidated vocabularies.","Linked Data application, Semantic MediaWiki, Singleton property, KnowledgeWiki, Wikidata, Open source, Semantic Web, Provenance metadata, Materials Science",
12/28/2020 23:45:23,2016,"Towards A Cache-Enabled, Order-Aware, Ontology-Based Stream Reasoning Framework",http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_13.pdf,Rui Yan,Tetherless World Constellation, US,Brenda Praggastis,Pacific Northwest National Laboratory, US,William P. Smith,Pacific Northwest National Laboratory, US,Deborah L. McGuinness,Tetherless World Constellation, US,,,,,,,,,,"While streaming data have become increasingly more popular in business and research communities, semantic models
and processing software for streaming data have not kept
pace. Traditional semantic solutions have not addressed
transient data streams. Semantic web languages (e.g., RDF,
OWL) have typically addressed static data settings and linked
data approaches have predominantly addressed static or growing data repositories. Streaming data settings have some
fundamental differences; in particular, data are consumed
on the fly and data may expire.
Stream reasoning, a combination of stream processing and
semantic reasoning, has emerged with the vision of providing ‚Äúsmart‚Äú processing of streaming data. C-SPARQL is
a prominent stream reasoning system that handles semantic (RDF) data streams. Many stream reasoning systems
including C-SPARQL use a sliding window and use data
arrival time to evict data. For data streams that include
expiration times, a simple arrival time scheme is inadequate
if the window size does not match the expiration period.
In this paper, we propose a cache-enabled, order-aware,
ontology-based stream reasoning framework. This framework consumes RDF streams with expiration timestamps
assigned by the streaming source. Our framework utilizes
both arrival and expiration timestamps in its cache eviction
policies. In addition, we introduce the notion of ‚Äúsemantic importance‚Äú which aims to address the relevance of data to the expected reasoning, thus enabling the eviction algorithms to be more context- and reasoning-aware when choosing what data to maintain for question answering. We evaluate this framework by implementing three different prototypes and utilizing five metrics. The trade-offs of deploying
the proposed framework are also discussed.","Data Cache, Order-awareness, Stream Reasoning, Semantic Web",C.1.3 [Other Architecture Styles]: Data-flow architectures‚Äîstream reasoning; D.2.11 [Software Architectures]: Patterns
12/28/2020 23:47:05,2016,R2RML-F: Towards Sharing and Executing Domain Logic in R2RML Mappings,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_14.pdf,Christophe Debruyne,Trinity College Dublin, Ireland,Declan O’Sullivan,Trinity College Dublin, Ireland,,,,,,,,,,,,,,,,"Many initiatives have emerged to aid one in publishing structured
resources as Linked Data on the Web with one of the major
achievements being the R2RML W3C Recommendation. R2RML
and its dialects assume a certain underlying technology (e.g., Core
SQL 2008). This means that domain-specific data transformations
‚Äì such as transforming geospatial coordinates ‚Äì rely either on that
underlying technology or data-preprocessing steps. We argue that
one can incorporate and subsequently share that procedural domain knowledge in such mappings. Such an extension would
make certain pre-processing steps redundant. One can furthermore
attach metadata to these functions, which can be published as
well. In this paper, we present R2RML-F, an extension to
R2RML, that adopts ECMAScript for capturing domain
knowledge and for which we have developed a prototype. We
demonstrate the viability of the approach with a demonstration
and compare its performance with different mappings in some
initial experiments. Our preliminary results suggest that there is
little or no overhead with respect to relying on underlying technology",R2RML; Linked Data; Mapping,
12/28/2020 23:51:11,2016,Semantic Hadith: Leveraging Linked Data Opportunities for Islamic Knowledge,http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_06.pdf,Amna Basharat,University of Georgia, US,Bushra Abro,Islamic International University, Pakistan,I. Budak Arpinar,University of Georgia, US,Khaled Rasheed,University of Georgia, US,,,,,,,,,,"While the linked data paradigm has gathered much attention over the recent years, the domain of Islamic knowledge
has yet to cache upon its full potential. The web-scale integration of Islamic texts and knowledge sources at large
is currently not well facilitated. The two primary sources
of the Islamic legislation are the Qur‚Äôan and the Hadith
(collections of Prophetic Narrations) and form the basis of
laying the foundation for anyone wanting to learn Islam.
This paper presents ongoing design and development efforts
to semantically model and publish the Hadith, which holds
a primary position as the next most important knowledge
source, after the Qur‚Äôan. We present the design of the linked
data vocabulary for not only publishing these narrations as
linked data, but also delineate upon the mechanism for linking these narrations with the verses of the Qur‚Äôan. We establish how the links between the Hadith and the Qur‚Äôanic
verses may be captured and published using this vocabulary, as derived from the secondary and tertiary sources of
knowledge. We present detailed insights into the potential,
the design considerations and the use cases of publishing this
wealth of knowledge as linked data.",linked data; hadith;Quran;Qur‚Äôan; semantic web; Islamic knowledge;,
12/28/2020 23:52:56,2017,Piecing the puzzle: Self-publishing queryable research data on the Web,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_4.pdf,Ruben Verborgh,Ghent University,Belgium,,,,,,,,,,,,,,,,,,,"Publishing research on the Web accompanied by machine-readable
data is one of the aims of Linked Research. Merely embedding
metadata as RDFa in HTML research articles, however, does not
solve the problems of accessing and querying that data. Hence,
I created a simple ETL pipeline to extract and enrich Linked Data
from my personal website, publishing the result in a queryable way
through Triple Pattern Fragments. The pipeline is open source, uses
existing ontologies, and can be adapted to other websites. In this article, I discuss this pipeline, the resulting data, and its possibilities
for query evaluation on the Web. More than 35,000 RDF triples of
my data are queryable, even with federated SPARQL queries because of links to external datasets. This proves that researchers do
not need to depend on centralized repositories for readily accessible
(meta-)data, but instead can‚Äîand should‚Äîtake matters into their
own hands.",,
12/28/2020 23:54:33,2017,deepschema.org: An Ontology for Typing Entities in the Web of Data,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_6.pdf,Panayiotis Smeros,EPFL, Switzerland,Amit Gupta,EPFL, Switzerland,Michele Catasta,EPFL, Switzerland,Karl Aberer,EPFL, Switzerland,,,,,,,,,,"Discovering the appropriate type of an entity in the Web of
Data is still considered an open challenge, given the complexity of the many tasks it entails. Among them, the most
notable is the definition of a generic and cross-domain ontology. While the ontologies proposed in the past function
mostly as schemata for knowledge bases of different sizes,
an ontology for entity typing requires a rich, accurate and
easily-traversable type hierarchy. Likewise, it is desirable
that the hierarchy contains thousands of nodes and multiple levels, contrary to what a manually curated ontology can
offer. Such level of detail is required to describe all the possible environments in which an entity exists in. Furthermore,
the generation of the ontology must follow an automated
fashion, combining the most widely used data sources and
following the speed of the Web.
In this paper we propose deepschema.org, the first ontology that combines two well-known ontological resources,
Wikidata and schema.org, to obtain a highly-accurate,
generic type ontology which is at the same time a firstclass citizen in the Web of Data. We describe the automated procedure we used for extracting a class hierarchy from Wikidata and analyze the main characteristics of
this hierarchy. We also provide a novel technique for integrating the extracted hierarchy with schema.org, which
exploits external dictionary corpora and is based on word
embeddings. Finally, we present a crowdsourcing evaluation which showcases the three main aspects of our ontology,
namely the accuracy, the traversability and the genericity.
The outcome of this paper is published under the portal:
http://deepschema.github.io","Class Hierarchy, Taxonomy, Ontology, Wikidata, schema.org, Data Extraction, Data Integration, Entity Typing",
12/28/2020 23:58:33,2017,Lifting Data Portals to the Web of Data Or: Linked Open Data (Portals) Now for real!,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_9.pdf,Sebastian Neumaier,Vienna University of Economics and Business, Austria,Jurgen Umbrich,Vienna University of Economics and Business, Austria,Axel Polleres,Vienna University of Economics and Business, Austria,,,,,,,,,,,,,"Data portals are central hubs for freely available (governmental)
datasets. ese portals use dierent soware frameworks to publish
their data, and the metadata descriptions of these datasets come in
dierent schemas accordingly to the framework. e present work
aims at re-exposing and connecting the metadata descriptions of
currently 854k datasets on 261 data portals to the Web of Linked
Data by mapping and publishing their homogenized metadata in
standard vocabularies such as DCAT and Schema.org. Additionally,
we publish existing quality information about the datasets and further enrich their descriptions by automatically generated metadata
for CSV resources. In order to make all this information traceable
and trustworthy, we annotate the generated data using the W3C‚Äôs
provenance vocabulary. e dataset descriptions are harvested
weekly and we oer access to the archived data by providing APIs
compliant to the Memento framework. All this data ‚Äì a total of
about 120 million triples per weekly snapshot ‚Äì is queryable at the
SPARQL endpoint at data.wu.ac.at/portalwatch/sparql.",,
12/29/2020 0:21:30,2017,Revisiting the Representation of and Need for Raw Geometries on the Linked Data Web,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_11.pdf,Blake Regalia,University of California, US,Krzysztof Janowicz,University of California, US,Grant McKenzie,University of Maryland, US,,,,,,,,,,,,,"Geospatial data on the Semantic Web historically stems from using
point geometries to represent the geographic locations of places. As
the practice evolved in the Semantic Web community, a demand for
more complex geometries and geospatial query capabilities came
about as a consequence of integrating traditional GIS and geo-data
into the Linked Data cloud. However, recent projects have revealed
that, in practice, these established techniques have major shortcomings that limit their storage, transmission, and query potential. In
this position paper, we examine these shortcomings, propose to
treat geometries similar to how other binary data are stored and
referenced on the Semantic Web, namely by representing them as
resources via URIs instead of RDF literals, and demonstrate the
utility of precomputing topological relations rather than computing them on-demand by arguing that end users are most oen
interested in topology and not raw geometries.",,
12/29/2020 0:25:30,2017,High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_1.pdf,Preeti Bhargava,Lithium Technologies, US,Nemanja Spasojevic,Lithium Technologies, US,Guoning Hu,Lithium Technologies, US,,,,,,,,,,,,,"The Entity Disambiguation and Linking (EDL) task matches
entity mentions in text to a unique Knowledge Base (KB)
identifier such as a Wikipedia or Freebase id. It plays a
critical role in the construction of a high quality information
network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization
and document tagging. EDL is a complex and challenging
problem due to ambiguity of the mentions and real world
text being multi-lingual. Moreover, EDL systems need to
have high throughput and should be lightweight in order
to scale to large datasets and run on off-the-shelf machines.
More importantly, these systems need to be able to extract
and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task
running on the data to be more efficient and accurate. In
order to address all these challenges, we present the Lithium
EDL system and algorithm - a high-throughput, lightweight,
language-agnostic EDL system that extracts and correctly
disambiguates 75% more entities than state-of-the-art EDL
systems and is significantly faster than them.","Entity Disambiguation, Entity Linking, Entity Resolution, Text Mining",
12/29/2020 0:29:43,2017,Client-side Processing of GeoSPARQL Functions with Triple Pattern Fragments,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_8.pdf,Christophe Debruyne,ADAPT Centre, Ireland,Eamonn Clinton,Ordnance Survey Ireland, Ireland,Declan O’Sullivan,ADAPT Centre, Ireland,,,,,,,,,,,,,"‚ÄúPlace‚Äù is an important concept providing a useful dimension to
explore, align and analyze data on the Linked Data Web. Though
Linked Data datasets can use standardized geospatial predicates
such as GeoSPARQL, access to SPARQL endpoints that supports
these is not guaranteed. When not available, one needs to load
the data into their own GeoSPARQL-enabled triplestores in order
to avail of those predicates. Triple Pattern Fragments (TPF) is a
proposal to make clients more intelligent in processing RDF, thereby
lessening the burden carried by servers. In this paper, we propose to
extend TPF to support GeoSPARQL. The contribution is a minimal
extension of the TPF client that does not rely on a spatial database
such that the extension can be run from within a browser. Even
though our approach will unlikely outperform GeoSPARQL-enabled
triplestores in terms of query execution time, we demonstrate its
feasibility by means of a couple of use cases using data provided
by data.geohive.ie, an initiative to publish authoritative, highresolution geospatial data for The Republic of Ireland as Linked Data
on the Web. This high-resolution data does cause a lot of network
trac, but related work showed how extending the communication
between a TPF client and server reduces the number HTTP calls
and some network traffic. The integration of our extension in one
such optimization did reduce the overhead. We, however, decided
to stick to our first implementation as it only extended the client
in a minimal way. Future work includes investigating how our
approach scales, and its usefulness of adding and using a spatial
component to datasets.","GeoSPARQL, Triple Paern Fragments, Ordnance Survey Ireland",
12/29/2020 0:31:02,2017,Learning to Identify Complementary Products from DBpedia,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_5.pdf,Victor Anthony Arrascue Ayala,University of Freiburg, Germany,Trong-Nghia Cheng,University of Freiburg, Germany,Anas Alzoghbi,University of Freiburg, Germany,Georg Lausen,University of Freiburg, Germany,,,,,,,,,,"Identifying the complementary relationship between products, like a cartridge to a printer, is a very useful technique
to provide recommendations. These are typically purchased
together or within a short time frame and thus online retailers benefit from it. Existing approaches rely heavily on
transactions and therefore they suffer from: (1) the cold
start problem for new products; (2) the inability to produce good results for infrequently bought products; (3) the
inability to explain why two products are complementary.
We propose a framework that aims at alleviating these
problems by exploiting a knowledge graph (DBpedia) in
addition to products‚Äô available information such as titles,
descriptions and categories rather than transactions. Our
problem is modeled as a classification task on a set of product pairs. Our starting point is the semantic paths in the
knowledge graph linking between product attributes, from
which we model product features. Then, having a labeled
set of product pairs we learn a model; and finally, we use this
model to predict complementary products for an unseen set
of products. Our experiments on a real world dataset from
Amazon show high performance of our framework in predicting whether one product is complementary to another
one.","Recommender Systems, Complementary Products, DBpedia, Knowledge-graph, Supervised Learning, Cross-domain",
12/29/2020 0:33:14,2017,Towards a Personalized Query Answering Framework on the Web of Data,http://events.linkeddata.org/ldow2017/papers/LDOW_2017_paper_7.pdf,Enayat Rajabi,Dalhousie University, Canada,Christophe Debruyne,Trinity College Dublin, Ireland,Declan O’Sullivan,Trinity College Dublin, Ireland,,,,,,,,,,,,,"In this paper, we argue that layering a question answering system
on the Web of Data based on user preferences, leads to the derivation of more knowledge from external sources and customisation
of query results based on user‚Äôs interests. As various users may find
different things relevant because of different preferences and goals,
we can expect different answers to the same query. We propose a
personalised question answering framework for a user to query
over Linked Data, which enhances a user query with related preferences of the user stored in his/her user profile with the aim of
providing personalized answers. We also propose the extension of
the QALD-5 scoring system to define a relevancy metric that
measures similarity of query answers to a user‚Äôs preferences.",Personalisation; Linked Data; Question Answering,
12/29/2020 0:35:44,2018,Practical Linked Data Access via SPARQL: The Case of Wikidata,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_4.pdf,Adrian Bielefeldt,Center for Advancing Electronics Dresden, Germany,Julius Gonsior,Center for Advancing Electronics Dresden, Germany,Markus Krötzsch,Center for Advancing Electronics Dresden, Germany,,,,,,,,,,,,,"SPARQL is one of the main APIs for accessing linked data collections. Compared to other modes of access, SPARQL queries carry
much more information on the precise information need of users,
and their analysis can therefore yield valuable insights into the practical usage of linked data sets. In this paper, we focus on Wikidata,
the knowledge-graph sister of Wikipedia, which offers linked data
exports and a heavily used SPARQL endpoint since 2015. Our detailed analysis of Wikidata‚Äôs server-side query logs reveals several
important differences to previously studied uses of SPARQL over
large knowledge graphs. Wikidata queries tend to be much more
complex and varied than queries observed elsewhere. Our analysis
is founded on a simple but effective separation of robotic from organic traffic. Whereas the robotic part is highly volatile and seems
unpredictable even on larger time scales, the much smaller organic
part shows clear trends in individual human usage. We analyse
query features, structure, and content to gather further evidence
that our approach is essential for obtaining meaningful results here.",,
12/29/2020 0:37:00,2018,SPARQL Micro-Services: Lightweight Integration of Web APIs and Linked Data,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_1.pdf,Franck Michel,University Côte d’Azur, France,Catherine Faron-Zucker,University Côte d’Azur, France,Fabien Gandon,University Côte d’Azur, France,,,,,,,,,,,,,"Web APIs are a prominent source of machine-readable information. We hypothesize that harnessing the Semantic Web standards
to enable automatic combination of Linked Data and non-RDF
Web APIs data could trigger novel cross-fertilization scenarios. To
achieve this goal, we define the SPARQL Micro-Service architecture.
A SPARQL micro-service is a lightweight, task-specific SPARQL
endpoint that provides access to a small, resource-centric, virtual
graph, while dynamically assigning dereferenceable URIs to Web
API resources that do not have URIs beforehand. The graph is delineated by the Web API service being wrapped, the arguments
passed to this service, and the restricted types of RDF triples that
this SPARQL micro-service is designed to spawn. In this context, we
argue that full SPARQL expressiveness can be supported efficiently
without jeopardizing servers availability. Eventually, we believe
that an ecosystem of SPARQL micro-services could emerge from
independent service providers, enabling Linked Data-based applications to glean pieces of data from a wealth of distributed, scalable
and reliable services. We describe an experimentation where we
dynamically augment biodiversity-related Linked Data with data
from Flickr, MusicBrainz and the Macauley scientific media library","Web API, SPARQL, micro-service, Data Integration, Linked Data, JSON-LD",
12/29/2020 0:38:20,2018,Rule-based Programming of User Agents for Linked Data,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_7.pdf,Tobias Käfer,Institute AIFB, Germany,Andreas Harth,Institute AIFB, Germany,,,,,,,,,,,,,,,,"While current Semantic Web languages and technologies are wellsuited for accessing and integrating static data, methods and technologies for the handling of dynamic aspects ‚Äì required in many
modern web environments ‚Äì are largely missing. We propose to use
Abstract State Machines (ASMs) as the formal basis for dealing with
changes in Linked Data, which is the combination of the Resource
Description Framework (RDF) with the Hypertext Transfer Protocol (HTTP). We provide a synthesis of ASMs and Linked Data and
show how the combination aligns with the relevant specifications
such as the Request/Response communication in HTTP, the guidelines for updating resource state in the Linked Data Platform (LDP)
specification, and the formal grounding of RDF in model theory.
Based on the formalisation of Linked Data resources that change
state over time, we present the syntax and operational semantics of
a small rule-based language to specify user agents that use HTTP
to interact with Linked Data as the interface to the environment.
We show the feasibility of the approach in an evaluation involving
the specification of automation in a Smart Building scenario, where
the presented approach serves as a theoretical foundation.",,
12/29/2020 0:39:18,2018,D2RML: Integrating Heterogeneous Data and Web Services into Custom RDF Graphs,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_11.pdf,Alexandros Chortaras,National Technical University of Athens, Greece,Giorgos Stamou,National Technical University of Athens, Greece,,,,,,,,,,,,,,,,"In this paper, we present the D2RML Data-to-RDF Mapping Language, as an extension of the R2RML mapping language, which
significantly enhances its abilities to collect data from diverse data
sources and transform them into custom RDF graphs. The definition of D2RML is based on a simple formal abstract data model,
which is needed to clearly define its semantics, given the diverse
types of data representation standards used in practice. D2RML
allows web service-based data transformations, simple data manipulation and filtering, and conditional maps, so as to improve
the selectivity of RDF mapping rules and facilitate the generation
of higher quality RDF data stores, through a lightweight, easy to
write and modify specification.","RDF mapping language, Data integration, Web service integration",
12/29/2020 0:41:16,2018,Evaluating Approaches for Supervised Semantic Labeling,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_6.pdf,Nataliia Rümmele,SIEMENS, Germany,Yuriy Tyshetskiy,"Data61, CSIRO", Australia,Alex Collins,"Data61, CSIRO", Australia,,,,,,,,,,,,,"Relational data sources are still one of the most popular
ways to store enterprise or Web data, however, the issue
with relational schema is the lack of a well-defined semantic
description. A common ontology provides a way to represent the meaning of a relational schema and can facilitate
the integration of heterogeneous data sources within a domain. Semantic labeling is achieved by mapping attributes
from the data sources to the classes and properties in the
ontology. We formulate this problem as a multi-class classification problem where previously labeled data sources are
used to learn rules for labeling new data sources. The majority of existing approaches for semantic labeling have focused
on data integration challenges such as naming conflicts and
semantic heterogeneity. In addition, machine learning approaches typically have issues around class imbalance, lack
of labeled instances and relative importance of attributes.
To address these issues, we develop a new machine learning
model with engineered features as well as two deep learning
models which do not require extensive feature engineering.
We evaluate our new approaches with the state-of-the-art.","data integration, schema matching, semantic labeling, ontology, relational schema, bagging",
12/29/2020 0:42:20,2018,GEEK: Incremental Graph-based Entity Disambiguation,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_8.pdf,Alexios Mandalios,National Technical University of Athens, Greece,Konstantinos Tzamaloukas,National Technical University of Athens, Greece,Alexandros Chortaras,National Technical University of Athens, Greece,Giorgos Stamou,National Technical University of Athens, Greece,,,,,,,,,,"A document may include mentions of people, locations, organizations, films, product brands and other kinds of entities. Such
mentions are often ambiguous, with no obvious way for a machine
to map them to real world entities, due to reasons like homonymy
and polysemy. The process of recognizing such mentions in unstructured texts and disambiguating them by mapping them to entities
stored in a knowledge base is known as Named Entity Recognition
and Disambiguation (NERD) or Entity Linking.
In this paper, we introduce GEEK (Graphical Entity Extraction
Kit), a NERD system that extracts named entities in text and links
them to a knowledge base using a graph-based method, taking
into account measures of entity commonness, relatedness, and
contextual similarity. All relevant data is retrieved at runtime using
public RESTful APIs. GEEK tries to push the performance limits
of a straightforward disambiguation method, that doesn‚Äôt require
arduous training or a complex mathematical foundation.","Named Entity Recognition, NER, Named Entity Disambiguation, NED, NERD, Google Knowledge Graph, Wikipedia, k-partite graph, max weight k-clique, worst out heuristic",
12/29/2020 0:45:24,2018,A Knowledge Base for Personal Information Management,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_3.pdf,David Montoya,Square Sense, France,Thomas Pellissier Tanon,"LTCI, Télécom ParisTech", France,Serge Abiteboul,Inria Paris, France,Pierre Senellart,Inria Paris, France,Fabian M. Suchanek,"LTCI, Télécom ParisTech", France,,,,,,,"Internet users have personal data spread over several devices and
across several web systems. In this paper, we introduce a novel
open-source framework for integrating the data of a user from
different sources into a single knowledge base. Our framework
integrates data of different kinds into a coherent whole, starting
with email messages, calendar, contacts, and location history. We
show how event periods in the user‚Äôs location data can be detected
and how they can be aligned with events from the calendar. This
allows users to query their personal information within and across
different dimensions, and to perform analytics over their emails,
events, and locations. Our system models data using RDF, extending
the schema.org vocabulary and providing a SPARQL interface.",,
12/29/2020 0:48:08,2018,What Factors Influence the Design of a Linked Data Generation Algorithm?,http://events.linkeddata.org/ldow2018/papers/LDOW2018_paper_12.pdf,Anastasia Dimou,Ghent University,Belgium,Pieter Heyvaert,Ghent University,Belgium,Ben De Meester,Ghent University,Belgium,Ruben Verborgh,Ghent University,Belgium,,,,,,,,,,"Generating Linked Data remains a complicated and intensive engineering process. While different factors determine how a Linked
Data generation algorithm is designed, potential alternatives for
each factor are currently not considered when designing the tools‚Äô
underlying algorithms. Certain design patterns are frequently applied across different tools, covering certain alternatives of a few of
these factors, whereas other alternatives are never explored. Consequently, there are no adequate tools for Linked Data generation
for certain occasions, or tools with inadequate and inefficient algorithms are chosen. In this position paper, we determine such factors,
based on our experiences, and present a preliminary list. These factors could be considered when a Linked Data generation algorithm
is designed or a tool is chosen. We investigated which factors are
covered by widely known Linked Data generation tools and concluded that only certain design patterns are frequently encountered.
By these means, we aim to point out that Linked Data generation
is above and beyond bare implementations, and algorithms need to
be thoroughly and systematically studied and exploited.",,